# ğŸ¤– æ„å»ºç®€å•RAGç³»ç»Ÿ

> **å­¦ä¹ ç›®æ ‡**ï¼šæ•´åˆEmbeddingã€å‘é‡æ•°æ®åº“å’ŒLLMï¼Œæ„å»ºå®Œæ•´çš„RAGé—®ç­”ç³»ç»Ÿ

---

## 1. RAGç³»ç»Ÿæ¶æ„

```
ç”¨æˆ·é—®é¢˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. é—®é¢˜å‘é‡åŒ–   â”‚ â† Embedding API
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. æ£€ç´¢ç›¸å…³æ–‡æ¡£ â”‚ â† å‘é‡æ•°æ®åº“
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. æ„å»ºPrompt  â”‚ â† é—®é¢˜ + æ£€ç´¢æ–‡æ¡£
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. LLMç”Ÿæˆå›ç­” â”‚ â† DeepSeek/GPT
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
      å›ç­”
```

---

## 2. å®Œæ•´å®ç°

### 2.1 å®‰è£…ä¾èµ–

```bash
pip install openai chromadb tiktoken
```

### 2.2 æ ¸å¿ƒRAGç±»

```python
import chromadb
from openai import OpenAI
import os

class SimpleRAG:
    def __init__(
        self,
        collection_name: str = "knowledge_base",
        persist_path: str = "./rag_db"
    ):
        # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
        self.chroma_client = chromadb.PersistentClient(path=persist_path)
        self.collection = self.chroma_client.get_or_create_collection(
            name=collection_name
        )
        
        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        self.llm_client = OpenAI(
            api_key=os.getenv("DEEPSEEK_API_KEY"),
            base_url="https://api.deepseek.com/v1"
        )
        
        # RAGé…ç½®
        self.top_k = 5
        self.max_context_length = 3000
    
    def add_documents(self, documents: list[str], ids: list[str] = None):
        """æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“"""
        if ids is None:
            ids = [f"doc_{i}" for i in range(len(documents))]
        
        self.collection.upsert(
            documents=documents,
            ids=ids
        )
        print(f"å·²æ·»åŠ  {len(documents)} ä¸ªæ–‡æ¡£ï¼Œæ€»æ•°: {self.collection.count()}")
    
    def retrieve(self, query: str) -> list[str]:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        results = self.collection.query(
            query_texts=[query],
            n_results=self.top_k,
            include=["documents", "distances"]
        )
        
        # è¿‡æ»¤ä½ç›¸ä¼¼åº¦ç»“æœ
        docs = []
        for doc, dist in zip(results["documents"][0], results["distances"][0]):
            if dist < 1.0:  # è·ç¦»é˜ˆå€¼
                docs.append(doc)
        
        return docs
    
    def _build_prompt(self, question: str, context_docs: list[str]) -> str:
        """æ„å»ºRAG Prompt"""
        context = "\n\n---\n\n".join(context_docs)
        
        # æˆªæ–­è¿‡é•¿çš„ä¸Šä¸‹æ–‡
        if len(context) > self.max_context_length:
            context = context[:self.max_context_length] + "..."
        
        prompt = f"""åŸºäºä»¥ä¸‹å‚è€ƒæ–‡æ¡£å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

## å‚è€ƒæ–‡æ¡£

{context}

## ç”¨æˆ·é—®é¢˜

{question}

## å›ç­”è¦æ±‚

1. åªæ ¹æ®å‚è€ƒæ–‡æ¡£ä¸­çš„ä¿¡æ¯å›ç­”
2. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜"æ ¹æ®ç°æœ‰èµ„æ–™æ— æ³•å›ç­”"
3. å›ç­”è¦ç®€æ´ã€å‡†ç¡®ã€æœ‰æ¡ç†
4. å¦‚æœå¯èƒ½ï¼Œå¼•ç”¨å…·ä½“çš„æ–‡æ¡£å†…å®¹

è¯·å›ç­”ï¼š"""
        
        return prompt
    
    def query(self, question: str) -> dict:
        """RAGé—®ç­”"""
        # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
        docs = self.retrieve(question)
        
        if not docs:
            return {
                "answer": "æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰æ‰¾åˆ°ä¸æ‚¨é—®é¢˜ç›¸å…³çš„ä¿¡æ¯ã€‚",
                "sources": [],
                "doc_count": 0
            }
        
        # 2. æ„å»ºPrompt
        prompt = self._build_prompt(question, docs)
        
        # 3. è°ƒç”¨LLMç”Ÿæˆå›ç­”
        response = self.llm_client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†åº“é—®ç­”åŠ©æ‰‹ï¼Œæ ¹æ®æä¾›çš„æ–‡æ¡£å›ç­”é—®é¢˜ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_tokens=1000
        )
        
        answer = response.choices[0].message.content
        
        return {
            "answer": answer,
            "sources": docs,
            "doc_count": len(docs)
        }

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    rag = SimpleRAG()
    
    # æ·»åŠ çŸ¥è¯†
    rag.add_documents([
        "FastAPIæ˜¯ä¸€ä¸ªç°ä»£ã€å¿«é€Ÿï¼ˆé«˜æ€§èƒ½ï¼‰çš„Webæ¡†æ¶ï¼Œç”¨äºæ„å»ºAPIã€‚å®ƒåŸºäºPython 3.7+çš„ç±»å‹æç¤ºã€‚",
        "FastAPIçš„æ€§èƒ½ä¸NodeJSå’ŒGoç›¸å½“ï¼Œæ˜¯Pythonæœ€å¿«çš„æ¡†æ¶ä¹‹ä¸€ã€‚",
        "FastAPIè‡ªåŠ¨ç”Ÿæˆäº¤äº’å¼APIæ–‡æ¡£ï¼ˆSwagger UIå’ŒReDocï¼‰ã€‚",
        "FastAPIä½¿ç”¨Pydanticè¿›è¡Œæ•°æ®éªŒè¯ï¼Œä½¿ç”¨Starletteä½œä¸ºWebéƒ¨åˆ†ã€‚",
        "Djangoæ˜¯ä¸€ä¸ªé«˜çº§Python Webæ¡†æ¶ï¼Œé¼“åŠ±å¿«é€Ÿå¼€å‘å’Œç®€æ´ã€åŠ¡å®çš„è®¾è®¡ã€‚"
    ])
    
    # é—®ç­”
    result = rag.query("FastAPIæœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ")
    print("å›ç­”:", result["answer"])
    print(f"å‚è€ƒäº† {result['doc_count']} ä¸ªæ–‡æ¡£")
```

---

## 3. æ·»åŠ æµå¼è¾“å‡º

```python
def query_stream(self, question: str):
    """æµå¼RAGé—®ç­”"""
    docs = self.retrieve(question)
    
    if not docs:
        yield "æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰æ‰¾åˆ°ä¸æ‚¨é—®é¢˜ç›¸å…³çš„ä¿¡æ¯ã€‚"
        return
    
    prompt = self._build_prompt(question, docs)
    
    response = self.llm_client.chat.completions.create(
        model="deepseek-chat",
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†åº“é—®ç­”åŠ©æ‰‹ã€‚"},
            {"role": "user", "content": prompt}
        ],
        temperature=0.3,
        stream=True  # å¯ç”¨æµå¼
    )
    
    for chunk in response:
        if chunk.choices[0].delta.content:
            yield chunk.choices[0].delta.content

# ä½¿ç”¨
for text in rag.query_stream("FastAPIæ˜¯ä»€ä¹ˆï¼Ÿ"):
    print(text, end="", flush=True)
```

---

## 4. æ·»åŠ FastAPIæ¥å£

```python
from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel

app = FastAPI(title="RAGé—®ç­”API")
rag = SimpleRAG()

class QueryRequest(BaseModel):
    question: str

class QueryResponse(BaseModel):
    answer: str
    sources: list[str]
    doc_count: int

class AddDocsRequest(BaseModel):
    documents: list[str]

@app.post("/query", response_model=QueryResponse)
async def query(request: QueryRequest):
    """é—®ç­”æ¥å£"""
    result = rag.query(request.question)
    return result

@app.post("/query/stream")
async def query_stream(request: QueryRequest):
    """æµå¼é—®ç­”æ¥å£"""
    def generate():
        for text in rag.query_stream(request.question):
            yield f"data: {text}\n\n"
    
    return StreamingResponse(generate(), media_type="text/event-stream")

@app.post("/documents")
async def add_documents(request: AddDocsRequest):
    """æ·»åŠ æ–‡æ¡£"""
    rag.add_documents(request.documents)
    return {"status": "success", "count": len(request.documents)}

@app.get("/documents/count")
async def get_doc_count():
    """è·å–æ–‡æ¡£æ•°é‡"""
    return {"count": rag.collection.count()}
```

---

## 5. æ–‡æ¡£é¢„å¤„ç†

### 5.1 ä»æ–‡ä»¶åŠ è½½

```python
def load_markdown(filepath: str) -> list[str]:
    """åŠ è½½Markdownæ–‡ä»¶å¹¶åˆ†å—"""
    with open(filepath, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # æŒ‰æ ‡é¢˜åˆ†å—
    import re
    sections = re.split(r'\n##\s+', content)
    
    chunks = []
    for section in sections:
        if section.strip():
            # è¿›ä¸€æ­¥åˆ†å‰²è¿‡é•¿çš„section
            if len(section) > 1000:
                paragraphs = section.split('\n\n')
                for p in paragraphs:
                    if p.strip():
                        chunks.append(p.strip())
            else:
                chunks.append(section.strip())
    
    return chunks

# ä½¿ç”¨
docs = load_markdown("./knowledge/fastapi_guide.md")
rag.add_documents(docs)
```

### 5.2 ä»ç›®å½•åŠ è½½

```python
import os

def load_directory(dirpath: str) -> list[str]:
    """åŠ è½½ç›®å½•ä¸‹æ‰€æœ‰Markdownæ–‡ä»¶"""
    all_docs = []
    
    for root, dirs, files in os.walk(dirpath):
        for file in files:
            if file.endswith('.md'):
                filepath = os.path.join(root, file)
                docs = load_markdown(filepath)
                all_docs.extend(docs)
    
    return all_docs

# ä½¿ç”¨
docs = load_directory("./knowledge")
rag.add_documents(docs)
```

---

## 6. å®Œæ•´é¡¹ç›®ç»“æ„

```
simple_rag/
â”œâ”€â”€ rag.py              # RAGæ ¸å¿ƒç±»
â”œâ”€â”€ api.py              # FastAPIæ¥å£
â”œâ”€â”€ loader.py           # æ–‡æ¡£åŠ è½½å™¨
â”œâ”€â”€ config.py           # é…ç½®
â”œâ”€â”€ knowledge/          # çŸ¥è¯†åº“æ–‡æ¡£
â”‚   â”œâ”€â”€ fastapi.md
â”‚   â”œâ”€â”€ python.md
â”‚   â””â”€â”€ ...
â”œâ”€â”€ rag_db/             # å‘é‡æ•°æ®åº“
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ“º æ¨èBç«™è§†é¢‘

æœç´¢ï¼š
- **"RAG å®æˆ˜ æ•™ç¨‹"**
- **"LangChain RAG å…¥é—¨"**
- **"çŸ¥è¯†åº“é—®ç­” Python"**

---

## 7. ç»§ç»­å­¦ä¹ 

ğŸ‰ **æ­å–œå®ŒæˆWeek 4ï¼**

ğŸ“Œ **Week 4 å­¦ä¹ é¡ºåº**ï¼š
1. âœ… Embeddingå‘é‡åŒ–å…¥é—¨
2. âœ… ChromaDBæˆ–Milvus
3. âœ… æ£€ç´¢ç­–ç•¥è¯¦è§£
4. âœ… æ„å»ºç®€å•RAGç³»ç»Ÿï¼ˆæœ¬æ•™ç¨‹ï¼‰

ç»§ç»­å‰å¾€ **Week 5** å­¦ä¹ RAGè¿›é˜¶æŠ€æœ¯ï¼

---

**ä½ å·²ç»æ„å»ºäº†ç¬¬ä¸€ä¸ªRAGç³»ç»Ÿï¼ğŸ’ª**
