# ğŸ“˜ æŸ¥è¯¢å˜æ¢æŠ€æœ¯ - HyDEä¸Multi-Query

> **å­¦ä¹ ç›®æ ‡**ï¼šæŒæ¡æŸ¥è¯¢å˜æ¢æŠ€æœ¯ï¼Œæå‡RAGæ£€ç´¢æ•ˆæœ

---

## ğŸ¯ ä¸ºä»€ä¹ˆéœ€è¦æŸ¥è¯¢å˜æ¢ï¼Ÿ

### ç”¨æˆ·æŸ¥è¯¢çš„é—®é¢˜

```
ç”¨æˆ·æŸ¥è¯¢: "FastAPIæ€ä¹ˆé™æµ"

é—®é¢˜:
1. æŸ¥è¯¢å¤ªç®€çŸ­ï¼Œè¯­ä¹‰ä¿¡æ¯ä¸è¶³
2. ä¸æ–‡æ¡£è¡¨è¿°ä¸åŒ¹é…ï¼ˆæ–‡æ¡£å¯èƒ½å†™"é€Ÿç‡é™åˆ¶"ã€"Rate Limiting"ï¼‰
3. å•ä¸€æŸ¥è¯¢å¯èƒ½é—æ¼ç›¸å…³ä¿¡æ¯

è§£å†³æ–¹æ¡ˆ: æŸ¥è¯¢å˜æ¢
```

---

## ğŸ“š ä¸‰ç§ä¸»è¦æŸ¥è¯¢å˜æ¢æŠ€æœ¯

### æŠ€æœ¯æ¦‚è§ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æŸ¥è¯¢å˜æ¢æŠ€æœ¯                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  1. Multi-Queryï¼ˆå¤šæŸ¥è¯¢æ‰©å±•ï¼‰                                     â”‚
â”‚     åŸå§‹æŸ¥è¯¢ â†’ ç”Ÿæˆå¤šä¸ªç›¸å…³æŸ¥è¯¢ â†’ åˆ†åˆ«æ£€ç´¢ â†’ åˆå¹¶ç»“æœ             â”‚
â”‚                                                                  â”‚
â”‚  2. HyDEï¼ˆå‡è®¾æ–‡æ¡£åµŒå…¥ï¼‰                                          â”‚
â”‚     åŸå§‹æŸ¥è¯¢ â†’ ç”Ÿæˆå‡è®¾ç­”æ¡ˆ â†’ ç”¨å‡è®¾ç­”æ¡ˆæ£€ç´¢ â†’ æ‰¾åˆ°çœŸå®æ–‡æ¡£       â”‚
â”‚                                                                  â”‚
â”‚  3. Step-Back Promptingï¼ˆåé€€æç¤ºï¼‰                               â”‚
â”‚     åŸå§‹æŸ¥è¯¢ â†’ ç”Ÿæˆæ›´æŠ½è±¡çš„é—®é¢˜ â†’ å…ˆå›ç­”æŠ½è±¡é—®é¢˜ â†’ å†å›ç­”åŸé—®é¢˜   â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’» Multi-Query å®ç°

### åŸç†

```
åŸå§‹æŸ¥è¯¢: "FastAPIæ€ä¹ˆé™æµ"
              â”‚
              â–¼
         LLMç”Ÿæˆå¤šä¸ªæŸ¥è¯¢:
         â”œâ”€â”€ "FastAPI rate limiting å®ç°æ–¹æ³•"
         â”œâ”€â”€ "FastAPI è¯·æ±‚é¢‘ç‡é™åˆ¶ä¸­é—´ä»¶"
         â”œâ”€â”€ "FastAPI slowapi ä½¿ç”¨æ•™ç¨‹"
         â””â”€â”€ "å¦‚ä½•é˜²æ­¢FastAPI APIè¢«æ»¥ç”¨"
              â”‚
              â–¼
       åˆ†åˆ«åœ¨å‘é‡åº“ä¸­æ£€ç´¢
              â”‚
              â–¼
         åˆå¹¶å»é‡ç»“æœ
```

### ä»£ç å®ç°

```python
from openai import OpenAI
from typing import Optional

class MultiQueryRetriever:
    """å¤šæŸ¥è¯¢æ£€ç´¢å™¨"""
    
    def __init__(self, client: OpenAI, vector_store, num_queries: int = 4):
        self.client = client
        self.vector_store = vector_store
        self.num_queries = num_queries
        
        self.generation_prompt = """ä½ æ˜¯ä¸€ä¸ªæŸ¥è¯¢æ‰©å±•ä¸“å®¶ã€‚

ç”¨æˆ·çš„åŸå§‹æŸ¥è¯¢: {query}

è¯·ç”Ÿæˆ{num}ä¸ªä¸åŒè§’åº¦çš„ç›¸å…³æŸ¥è¯¢ï¼Œç”¨äºåœ¨çŸ¥è¯†åº“ä¸­æœç´¢æ›´å…¨é¢çš„ä¿¡æ¯ã€‚
è¦æ±‚:
1. æ¯ä¸ªæŸ¥è¯¢åº”è¯¥ä»ä¸åŒè§’åº¦æè¿°åŒä¸€ä¸ªé—®é¢˜
2. ä½¿ç”¨ä¸åŒçš„å…³é”®è¯å’Œè¡¨è¿°æ–¹å¼
3. åŒ…å«å¯èƒ½çš„åŒä¹‰è¯å’Œç›¸å…³æœ¯è¯­

è¿”å›JSONæ ¼å¼:
{{"queries": ["æŸ¥è¯¢1", "æŸ¥è¯¢2", ...]}}
"""
    
    async def generate_queries(self, query: str) -> list[str]:
        """ç”Ÿæˆå¤šä¸ªæŸ¥è¯¢"""
        response = await self.client.chat.completions.create(
            model="deepseek-chat",
            messages=[{
                "role": "user",
                "content": self.generation_prompt.format(
                    query=query,
                    num=self.num_queries
                )
            }],
            response_format={"type": "json_object"},
            temperature=0.7  # é€‚åº¦åˆ›é€ æ€§
        )
        
        import json
        result = json.loads(response.choices[0].message.content)
        return [query] + result.get("queries", [])  # åŒ…å«åŸå§‹æŸ¥è¯¢
    
    async def retrieve(self, query: str, top_k: int = 5) -> list[dict]:
        """æ‰§è¡Œå¤šæŸ¥è¯¢æ£€ç´¢"""
        # ç”Ÿæˆå¤šä¸ªæŸ¥è¯¢
        queries = await self.generate_queries(query)
        
        # å¯¹æ¯ä¸ªæŸ¥è¯¢è¿›è¡Œæ£€ç´¢
        all_results = {}
        
        for q in queries:
            results = self.vector_store.query(
                query_texts=[q],
                n_results=top_k,
                include=["documents", "metadatas", "distances"]
            )
            
            # åˆå¹¶ç»“æœï¼ˆä½¿ç”¨æ–‡æ¡£IDå»é‡ï¼‰
            for i, doc_id in enumerate(results["ids"][0]):
                if doc_id not in all_results:
                    all_results[doc_id] = {
                        "id": doc_id,
                        "document": results["documents"][0][i],
                        "metadata": results["metadatas"][0][i] if results["metadatas"] else {},
                        "min_distance": results["distances"][0][i],
                        "matched_queries": [q]
                    }
                else:
                    # æ›´æ–°æœ€å°è·ç¦»å’ŒåŒ¹é…æŸ¥è¯¢
                    all_results[doc_id]["min_distance"] = min(
                        all_results[doc_id]["min_distance"],
                        results["distances"][0][i]
                    )
                    all_results[doc_id]["matched_queries"].append(q)
        
        # æŒ‰åŒ¹é…æŸ¥è¯¢æ•°é‡å’Œè·ç¦»æ’åº
        sorted_results = sorted(
            all_results.values(),
            key=lambda x: (-len(x["matched_queries"]), x["min_distance"])
        )
        
        return sorted_results[:top_k]


# ä½¿ç”¨ç¤ºä¾‹
async def demo_multi_query():
    client = OpenAI(base_url="https://api.deepseek.com/v1", api_key="your-key")
    
    retriever = MultiQueryRetriever(client, vector_store)
    
    results = await retriever.retrieve("FastAPIæ€ä¹ˆé™æµ")
    
    print("ç”Ÿæˆçš„æŸ¥è¯¢:")
    for r in results[:3]:
        print(f"  æ–‡æ¡£: {r['document'][:100]}...")
        print(f"  åŒ¹é…æŸ¥è¯¢æ•°: {len(r['matched_queries'])}")
```

---

## ğŸ’» HyDE å®ç°

### åŸç†

```
åŸå§‹æŸ¥è¯¢: "FastAPIæ€ä¹ˆé™æµ"
              â”‚
              â–¼
         LLMç”Ÿæˆå‡è®¾ç­”æ¡ˆ:
         "FastAPIå¯ä»¥ä½¿ç”¨slowapiåº“å®ç°é™æµã€‚é¦–å…ˆpip install slowapiï¼Œ
          ç„¶ååˆ›å»ºLimiterå®ä¾‹..."
              â”‚
              â–¼
         ç”¨å‡è®¾ç­”æ¡ˆçš„Embeddingæ£€ç´¢
         (å‡è®¾ç­”æ¡ˆçš„è¯­ä¹‰æ›´æ¥è¿‘çœŸå®æ–‡æ¡£)
              â”‚
              â–¼
         è¿”å›çœŸå®æ–‡æ¡£
```

### ä»£ç å®ç°

```python
class HyDERetriever:
    """HyDE (Hypothetical Document Embeddings) æ£€ç´¢å™¨"""
    
    def __init__(self, client: OpenAI, vector_store, embedding_model=None):
        self.client = client
        self.vector_store = vector_store
        self.embedding_model = embedding_model
        
        self.hyde_prompt = """ä½ æ˜¯ä¸€ä¸ªæŠ€æœ¯æ–‡æ¡£ä¸“å®¶ã€‚

ç”¨æˆ·é—®é¢˜: {query}

è¯·å†™ä¸€æ®µæŠ€æœ¯æ–‡æ¡£å†…å®¹ï¼Œç›´æ¥å›ç­”è¿™ä¸ªé—®é¢˜ã€‚
è¦æ±‚:
1. å†™æ³•è¦åƒçœŸæ­£çš„æŠ€æœ¯æ–‡æ¡£/æ•™ç¨‹
2. åŒ…å«å…·ä½“çš„æŠ€æœ¯ç»†èŠ‚å’Œä»£ç ç¤ºä¾‹
3. ä¸è¦ä»¥é—®ç­”å½¢å¼ï¼Œè€Œæ˜¯ä»¥æ•™ç¨‹/æ–‡æ¡£å½¢å¼

æ–‡æ¡£å†…å®¹:"""
    
    async def generate_hypothetical_document(self, query: str) -> str:
        """ç”Ÿæˆå‡è®¾æ–‡æ¡£"""
        response = await self.client.chat.completions.create(
            model="deepseek-chat",
            messages=[{
                "role": "user",
                "content": self.hyde_prompt.format(query=query)
            }],
            max_tokens=500,
            temperature=0.7
        )
        return response.choices[0].message.content
    
    async def get_embedding(self, text: str) -> list[float]:
        """è·å–æ–‡æœ¬å‘é‡"""
        response = await self.client.embeddings.create(
            model="text-embedding-3-small",  # æˆ–å…¶ä»–embeddingæ¨¡å‹
            input=text
        )
        return response.data[0].embedding
    
    async def retrieve(self, query: str, top_k: int = 5) -> list[dict]:
        """ä½¿ç”¨HyDEè¿›è¡Œæ£€ç´¢"""
        # Step 1: ç”Ÿæˆå‡è®¾æ–‡æ¡£
        hypothetical_doc = await self.generate_hypothetical_document(query)
        
        # Step 2: ç”¨å‡è®¾æ–‡æ¡£è¿›è¡Œæ£€ç´¢
        # ChromaDBä¼šè‡ªåŠ¨è®¡ç®—embedding
        results = self.vector_store.query(
            query_texts=[hypothetical_doc],
            n_results=top_k,
            include=["documents", "metadatas", "distances"]
        )
        
        return {
            "hypothetical_document": hypothetical_doc,
            "retrieved_documents": [
                {
                    "document": results["documents"][0][i],
                    "metadata": results["metadatas"][0][i] if results["metadatas"] else {},
                    "distance": results["distances"][0][i]
                }
                for i in range(len(results["documents"][0]))
            ]
        }


# ä½¿ç”¨ç¤ºä¾‹
async def demo_hyde():
    client = OpenAI(base_url="https://api.deepseek.com/v1", api_key="your-key")
    
    retriever = HyDERetriever(client, vector_store)
    
    result = await retriever.retrieve("FastAPIæ€ä¹ˆé™æµ")
    
    print("å‡è®¾æ–‡æ¡£:")
    print(result["hypothetical_document"][:300])
    print("\næ£€ç´¢åˆ°çš„çœŸå®æ–‡æ¡£:")
    for doc in result["retrieved_documents"][:2]:
        print(f"  - {doc['document'][:100]}...")
```

---

## ğŸ’» Step-Back Prompting å®ç°

### åŸç†

```
åŸå§‹æŸ¥è¯¢: "FastAPI 0.100.0ç‰ˆæœ¬çš„è·¯ç”±è£…é¥°å™¨æœ‰ä»€ä¹ˆå˜åŒ–"
              â”‚
              â–¼
         ç”ŸæˆæŠ½è±¡é—®é¢˜:
         "FastAPIè·¯ç”±ç³»ç»Ÿçš„è®¾è®¡å’Œæ¼”è¿›"
              â”‚
              â–¼
         å…ˆæ£€ç´¢æŠ½è±¡é—®é¢˜ç›¸å…³æ–‡æ¡£
         è·å¾—èƒŒæ™¯çŸ¥è¯†
              â”‚
              â–¼
         ç»“åˆèƒŒæ™¯çŸ¥è¯†å›ç­”åŸå§‹å…·ä½“é—®é¢˜
```

### ä»£ç å®ç°

```python
class StepBackRetriever:
    """Step-Back Prompting æ£€ç´¢å™¨"""
    
    def __init__(self, client: OpenAI, vector_store):
        self.client = client
        self.vector_store = vector_store
        
        self.stepback_prompt = """ä½ æ˜¯ä¸€ä¸ªé—®é¢˜æŠ½è±¡ä¸“å®¶ã€‚

åŸå§‹é—®é¢˜: {query}

è¯·å°†è¿™ä¸ªå…·ä½“é—®é¢˜æŠ½è±¡ä¸ºä¸€ä¸ªæ›´ä¸€èˆ¬æ€§çš„é—®é¢˜ã€‚
è¿™ä¸ªæŠ½è±¡é—®é¢˜åº”è¯¥èƒ½å¸®åŠ©ç†è§£åŸå§‹é—®é¢˜æ‰€éœ€çš„èƒŒæ™¯çŸ¥è¯†ã€‚

ä¾‹å¦‚:
- "Python 3.11çš„matchè¯­å¥æ€§èƒ½å¦‚ä½•" â†’ "Pythonæ¨¡å¼åŒ¹é…çš„è®¾è®¡å’Œå®ç°åŸç†"
- "React 18çš„useTransitionæ€ä¹ˆç”¨" â†’ "Reactå¹¶å‘æ¸²æŸ“çš„æ¦‚å¿µå’Œæœºåˆ¶"

è¿”å›JSONæ ¼å¼:
{{"abstract_question": "æŠ½è±¡åçš„é—®é¢˜"}}
"""
    
    async def generate_stepback_question(self, query: str) -> str:
        """ç”ŸæˆæŠ½è±¡é—®é¢˜"""
        response = await self.client.chat.completions.create(
            model="deepseek-chat",
            messages=[{
                "role": "user",
                "content": self.stepback_prompt.format(query=query)
            }],
            response_format={"type": "json_object"},
            temperature=0
        )
        
        import json
        result = json.loads(response.choices[0].message.content)
        return result.get("abstract_question", query)
    
    async def retrieve(self, query: str, top_k: int = 5) -> dict:
        """æ‰§è¡ŒStep-Backæ£€ç´¢"""
        # ç”ŸæˆæŠ½è±¡é—®é¢˜
        abstract_query = await self.generate_stepback_question(query)
        
        # æ£€ç´¢æŠ½è±¡é—®é¢˜çš„ç›¸å…³æ–‡æ¡£ï¼ˆèƒŒæ™¯çŸ¥è¯†ï¼‰
        background_results = self.vector_store.query(
            query_texts=[abstract_query],
            n_results=top_k // 2
        )
        
        # æ£€ç´¢åŸå§‹é—®é¢˜çš„ç›¸å…³æ–‡æ¡£ï¼ˆå…·ä½“ä¿¡æ¯ï¼‰
        specific_results = self.vector_store.query(
            query_texts=[query],
            n_results=top_k // 2
        )
        
        return {
            "original_query": query,
            "stepback_query": abstract_query,
            "background_docs": background_results["documents"][0],
            "specific_docs": specific_results["documents"][0]
        }
```

---

## ğŸ¯ æŠ€æœ¯é€‰æ‹©æŒ‡å—

| åœºæ™¯ | æ¨èæŠ€æœ¯ | åŸå›  |
|-----|---------|------|
| ç”¨æˆ·è¡¨è¿°å¤šæ · | Multi-Query | è¦†ç›–ä¸åŒè¡¨è¿°æ–¹å¼ |
| æŸ¥è¯¢ä¸æ–‡æ¡£è¡¨è¿°å·®å¼‚å¤§ | HyDE | å‡è®¾ç­”æ¡ˆæ›´æ¥è¿‘æ–‡æ¡£ |
| éœ€è¦èƒŒæ™¯çŸ¥è¯† | Step-Back | å…ˆç†è§£æ¦‚å¿µå†å›ç­” |
| ç»¼åˆä½¿ç”¨ | ç»„åˆæ–¹æ¡ˆ | å–é•¿è¡¥çŸ­ |

---

## ğŸ’» ç»„åˆä½¿ç”¨ç¤ºä¾‹

```python
class AdvancedRetriever:
    """é«˜çº§æ£€ç´¢å™¨ - ç»„åˆå¤šç§æŸ¥è¯¢å˜æ¢æŠ€æœ¯"""
    
    def __init__(self, client: OpenAI, vector_store):
        self.multi_query = MultiQueryRetriever(client, vector_store)
        self.hyde = HyDERetriever(client, vector_store)
        self.stepback = StepBackRetriever(client, vector_store)
    
    async def retrieve(
        self, 
        query: str, 
        strategy: str = "auto",
        top_k: int = 5
    ) -> list[str]:
        """æ™ºèƒ½é€‰æ‹©æ£€ç´¢ç­–ç•¥"""
        
        if strategy == "auto":
            # æ ¹æ®æŸ¥è¯¢ç‰¹ç‚¹è‡ªåŠ¨é€‰æ‹©
            if len(query) < 10:
                strategy = "multi_query"  # çŸ­æŸ¥è¯¢ç”¨å¤šæŸ¥è¯¢æ‰©å±•
            elif "ç‰ˆæœ¬" in query or "å˜åŒ–" in query:
                strategy = "stepback"     # ç‰ˆæœ¬ç›¸å…³ç”¨Step-Back
            else:
                strategy = "hyde"          # é»˜è®¤ç”¨HyDE
        
        if strategy == "multi_query":
            results = await self.multi_query.retrieve(query, top_k)
            return [r["document"] for r in results]
        
        elif strategy == "hyde":
            results = await self.hyde.retrieve(query, top_k)
            return [r["document"] for r in results["retrieved_documents"]]
        
        elif strategy == "stepback":
            results = await self.stepback.retrieve(query, top_k)
            return results["background_docs"] + results["specific_docs"]
        
        else:
            raise ValueError(f"æœªçŸ¥ç­–ç•¥: {strategy}")
```

---

## ğŸ“Š å­¦ä¹ æ£€æŸ¥æ¸…å•

- [ ] ç†è§£ç”¨æˆ·æŸ¥è¯¢ä¸æ–‡æ¡£ä¸åŒ¹é…çš„é—®é¢˜
- [ ] æŒæ¡Multi-Queryçš„åŸç†å’Œå®ç°
- [ ] æŒæ¡HyDEï¼ˆå‡è®¾æ–‡æ¡£åµŒå…¥ï¼‰çš„åŸç†å’Œå®ç°
- [ ] äº†è§£Step-Back Promptingçš„ä½¿ç”¨åœºæ™¯
- [ ] èƒ½å¤Ÿæ ¹æ®åœºæ™¯é€‰æ‹©åˆé€‚çš„æŸ¥è¯¢å˜æ¢æŠ€æœ¯

---

## ğŸ¯ ä¸‹ä¸€æ­¥

ç»§ç»­å­¦ä¹ ï¼š
ğŸ‘‰ [çˆ¶æ–‡æ¡£æ£€ç´¢å™¨](./06_parent_document_retriever.md)
