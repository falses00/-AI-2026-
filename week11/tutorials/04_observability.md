# ğŸ“Š å¯è§‚æµ‹æ€§å®æˆ˜ï¼šLangFuse + Prometheus

> **å­¦ä¹ ç›®æ ‡**ï¼šæŒæ¡ä¼ä¸šçº§AIç³»ç»Ÿçš„å¯è§‚æµ‹æ€§å®ç°

---

## 1. ä¸ºä»€ä¹ˆéœ€è¦å¯è§‚æµ‹æ€§ï¼Ÿ

åœ¨ç”Ÿäº§ç¯å¢ƒä¸­è¿è¡ŒAIç³»ç»Ÿï¼Œä½ éœ€è¦å›ç­”ï¼š

- â“ è¿™æ¬¡è¯·æ±‚ä¸ºä»€ä¹ˆå“åº”è¿™ä¹ˆæ…¢ï¼Ÿ
- â“ æ¨¡å‹è¿”å›çš„è´¨é‡å¦‚ä½•ï¼Ÿ
- â“ æ¯æœˆèŠ±äº†å¤šå°‘APIè´¹ç”¨ï¼Ÿ
- â“ å“ªäº›è¯·æ±‚å¯¼è‡´äº†é”™è¯¯ï¼Ÿ

**å¯è§‚æµ‹æ€§ = è¿½è¸ª + æŒ‡æ ‡ + æ—¥å¿—**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AIç³»ç»Ÿå¯è§‚æµ‹æ€§æ¶æ„                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚   â”‚   è¿½è¸ª         â”‚   â”‚   æŒ‡æ ‡         â”‚   â”‚   æ—¥å¿—         â”‚    â”‚
â”‚   â”‚   (Traces)     â”‚   â”‚   (Metrics)   â”‚   â”‚   (Logs)       â”‚    â”‚
â”‚   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚
â”‚   â”‚ â€¢ LangFuse    â”‚   â”‚ â€¢ Prometheus  â”‚   â”‚ â€¢ structlog   â”‚    â”‚
â”‚   â”‚ â€¢ LangSmith   â”‚   â”‚ â€¢ Grafana     â”‚   â”‚ â€¢ ELK Stack   â”‚    â”‚
â”‚   â”‚ â€¢ Arize       â”‚   â”‚               â”‚   â”‚               â”‚    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. LangFuseï¼šLLMå…¨é“¾è·¯è¿½è¸ª

### 2.1 å®‰è£…å’Œé…ç½®

```bash
pip install langfuse openai
```

```python
# ç¯å¢ƒå˜é‡é…ç½®
# LANGFUSE_PUBLIC_KEY=pk-lf-xxx
# LANGFUSE_SECRET_KEY=sk-lf-xxx
# LANGFUSE_HOST=https://cloud.langfuse.com (æˆ–è‡ªæ‰˜ç®¡åœ°å€)
```

### 2.2 åŸºç¡€ä½¿ç”¨ï¼šè£…é¥°å™¨è¿½è¸ª

```python
from langfuse.decorators import observe, langfuse_context
from openai import OpenAI

client = OpenAI()

@observe()
def chat(user_message: str) -> str:
    """å¸¦è¿½è¸ªçš„å¯¹è¯å‡½æ•°"""
    
    # æ·»åŠ è‡ªå®šä¹‰å…ƒæ•°æ®
    langfuse_context.update_current_trace(
        user_id="user_123",
        session_id="session_456",
        tags=["production", "chat"]
    )
    
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹"},
            {"role": "user", "content": user_message}
        ]
    )
    
    return response.choices[0].message.content

# ä½¿ç”¨
result = chat("ä»€ä¹ˆæ˜¯RAGï¼Ÿ")
```

### 2.3 å¤šæ­¥éª¤è¿½è¸ªï¼šAgentå·¥ä½œæµ

```python
from langfuse.decorators import observe
from langfuse import Langfuse

langfuse = Langfuse()

@observe(name="multi_step_agent")
async def agent_workflow(task: str):
    """å¤šæ­¥éª¤Agentå·¥ä½œæµè¿½è¸ª"""
    
    # Step 1: è§„åˆ’
    with langfuse_context.observe_as("planning") as span:
        plan = await plan_task(task)
        span.update(output=plan)
    
    # Step 2: æ‰§è¡Œ
    with langfuse_context.observe_as("execution") as span:
        result = await execute_plan(plan)
        span.update(
            output=result,
            metadata={"steps": len(plan)}
        )
    
    # Step 3: éªŒè¯
    with langfuse_context.observe_as("verification") as span:
        verified = await verify_result(result)
        span.update(
            output=verified,
            level="WARNING" if not verified["passed"] else "INFO"
        )
    
    # è®°å½•è¯„åˆ†
    langfuse_context.score_current_trace(
        name="quality",
        value=verified["score"],
        comment="è‡ªåŠ¨è´¨é‡è¯„åˆ†"
    )
    
    return verified

async def plan_task(task: str) -> list:
    """è§„åˆ’ä»»åŠ¡"""
    response = await client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": f"åˆ†è§£ä»»åŠ¡: {task}"}]
    )
    return parse_plan(response.choices[0].message.content)

async def execute_plan(plan: list) -> dict:
    """æ‰§è¡Œè®¡åˆ’"""
    results = []
    for step in plan:
        result = await execute_step(step)
        results.append(result)
    return {"steps": results}

async def verify_result(result: dict) -> dict:
    """éªŒè¯ç»“æœ"""
    return {"passed": True, "score": 0.9}
```

### 2.4 RAGè¿½è¸ª

```python
@observe()
async def rag_query(question: str) -> str:
    """RAGæŸ¥è¯¢è¿½è¸ª"""
    
    # è¿½è¸ªæ£€ç´¢æ­¥éª¤
    with langfuse_context.observe_as("retrieval", type="retriever") as span:
        docs = await vector_store.similarity_search(question, k=5)
        span.update(
            input=question,
            output=[doc.page_content[:100] for doc in docs],
            metadata={"num_docs": len(docs)}
        )
    
    # è¿½è¸ªç”Ÿæˆæ­¥éª¤
    with langfuse_context.observe_as("generation", type="llm") as span:
        context = "\n".join([doc.page_content for doc in docs])
        
        response = await client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": f"æ ¹æ®ä»¥ä¸‹å†…å®¹å›ç­”:\n{context}"},
                {"role": "user", "content": question}
            ]
        )
        
        answer = response.choices[0].message.content
        span.update(
            input={"question": question, "context_length": len(context)},
            output=answer,
            usage={
                "input": response.usage.prompt_tokens,
                "output": response.usage.completion_tokens
            }
        )
    
    return answer
```

---

## 3. Prometheusï¼šæŒ‡æ ‡ç›‘æ§

### 3.1 å®šä¹‰æŒ‡æ ‡

```python
from prometheus_client import Counter, Histogram, Gauge, start_http_server

# è®¡æ•°å™¨ï¼šç´¯ç§¯å€¼
llm_requests_total = Counter(
    'llm_requests_total',
    'LLMè¯·æ±‚æ€»æ•°',
    ['model', 'endpoint', 'status']
)

# ç›´æ–¹å›¾ï¼šåˆ†å¸ƒï¼ˆå¦‚å»¶è¿Ÿï¼‰
llm_latency_seconds = Histogram(
    'llm_latency_seconds',
    'LLMå“åº”å»¶è¿Ÿï¼ˆç§’ï¼‰',
    ['model'],
    buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0]
)

# è®¡æ•°å™¨ï¼šTokenç”¨é‡
llm_tokens_total = Counter(
    'llm_tokens_total',
    'Tokenä½¿ç”¨é‡',
    ['model', 'type']  # type: input/output
)

# ä»ªè¡¨ï¼šå½“å‰å€¼
active_sessions = Gauge(
    'active_sessions',
    'å½“å‰æ´»è·ƒä¼šè¯æ•°'
)
```

### 3.2 åœ¨ä»£ç ä¸­è®°å½•æŒ‡æ ‡

```python
import time

async def tracked_chat(message: str, model: str = "gpt-4o-mini"):
    """å¸¦æŒ‡æ ‡è¿½è¸ªçš„å¯¹è¯"""
    
    start_time = time.time()
    active_sessions.inc()
    
    try:
        response = await client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": message}]
        )
        
        # è®°å½•æˆåŠŸ
        llm_requests_total.labels(
            model=model,
            endpoint="chat",
            status="success"
        ).inc()
        
        # è®°å½•Token
        llm_tokens_total.labels(model=model, type="input").inc(
            response.usage.prompt_tokens
        )
        llm_tokens_total.labels(model=model, type="output").inc(
            response.usage.completion_tokens
        )
        
        return response.choices[0].message.content
        
    except Exception as e:
        llm_requests_total.labels(
            model=model,
            endpoint="chat",
            status="error"
        ).inc()
        raise
        
    finally:
        # è®°å½•å»¶è¿Ÿ
        duration = time.time() - start_time
        llm_latency_seconds.labels(model=model).observe(duration)
        active_sessions.dec()
```

### 3.3 FastAPIé›†æˆ

```python
from fastapi import FastAPI
from prometheus_client import make_asgi_app

app = FastAPI()

# æŒ‚è½½PrometheusæŒ‡æ ‡ç«¯ç‚¹
metrics_app = make_asgi_app()
app.mount("/metrics", metrics_app)

@app.get("/chat")
async def chat_endpoint(message: str):
    result = await tracked_chat(message)
    return {"response": result}
```

### 3.4 Prometheusé…ç½®

```yaml
# prometheus.yml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'ai-assistant'
    static_configs:
      - targets: ['localhost:8000']
    metrics_path: /metrics
```

---

## 4. Grafanaä»ªè¡¨æ¿

### 4.1 æ ¸å¿ƒé¢æ¿

**è¯·æ±‚é€Ÿç‡**ï¼š
```promql
rate(llm_requests_total[5m])
```

**å¹³å‡å»¶è¿Ÿ**ï¼š
```promql
histogram_quantile(0.95, rate(llm_latency_seconds_bucket[5m]))
```

**Tokenä½¿ç”¨**ï¼š
```promql
sum(rate(llm_tokens_total[1h])) by (model, type)
```

**é”™è¯¯ç‡**ï¼š
```promql
rate(llm_requests_total{status="error"}[5m]) / rate(llm_requests_total[5m]) * 100
```

### 4.2 å‘Šè­¦è§„åˆ™

```yaml
# alerts.yml
groups:
  - name: ai-alerts
    rules:
      - alert: HighLatency
        expr: histogram_quantile(0.95, rate(llm_latency_seconds_bucket[5m])) > 5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "LLMå“åº”å»¶è¿Ÿè¿‡é«˜"
          
      - alert: HighErrorRate
        expr: rate(llm_requests_total{status="error"}[5m]) / rate(llm_requests_total[5m]) > 0.05
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "LLMé”™è¯¯ç‡è¶…è¿‡5%"
```

---

## 5. æˆæœ¬è¿½è¸ª

```python
# å„æ¨¡å‹å®šä»· (ç¾å…ƒ/1K tokens)
MODEL_PRICING = {
    "gpt-4o": {"input": 0.005, "output": 0.015},
    "gpt-4o-mini": {"input": 0.00015, "output": 0.0006},
    "gpt-4-turbo": {"input": 0.01, "output": 0.03},
    "deepseek-chat": {"input": 0.00014, "output": 0.00028},
}

llm_cost_dollars = Counter(
    'llm_cost_dollars',
    'LLMè°ƒç”¨æˆæœ¬ï¼ˆç¾å…ƒï¼‰',
    ['model']
)

def calculate_cost(model: str, input_tokens: int, output_tokens: int) -> float:
    """è®¡ç®—APIè°ƒç”¨æˆæœ¬"""
    pricing = MODEL_PRICING.get(model, {"input": 0.001, "output": 0.002})
    cost = (
        input_tokens * pricing["input"] / 1000 +
        output_tokens * pricing["output"] / 1000
    )
    llm_cost_dollars.labels(model=model).inc(cost)
    return cost
```

---

## 6. å­¦ä¹ æ£€æŸ¥æ¸…å•

- [ ] èƒ½å¤Ÿé…ç½®LangFuseè¿½è¸ª
- [ ] ä¼šä½¿ç”¨è£…é¥°å™¨è¿½è¸ªAgentå·¥ä½œæµ
- [ ] ç†è§£PrometheusæŒ‡æ ‡ç±»å‹ï¼ˆCounter/Histogram/Gaugeï¼‰
- [ ] èƒ½å¤Ÿåˆ›å»ºGrafanaä»ªè¡¨æ¿
- [ ] ä¼šè®¾ç½®å‘Šè­¦è§„åˆ™
- [ ] èƒ½å¤Ÿè¿½è¸ªTokenæˆæœ¬

---

## ç»§ç»­å­¦ä¹ 

ğŸ“Œ **æ¨èé¡ºåº**ï¼š
1. âœ… å¯è§‚æµ‹æ€§å®æˆ˜ï¼ˆæœ¬æ•™ç¨‹ï¼‰
2. ğŸ”œ [Guardrailså®‰å…¨æŠ¤æ ](./05_guardrails.md)

---

**æ²¡æœ‰å¯è§‚æµ‹æ€§çš„AIç³»ç»Ÿæ˜¯é£è¡Œç›²åŒºï¼ğŸ‘ï¸**
