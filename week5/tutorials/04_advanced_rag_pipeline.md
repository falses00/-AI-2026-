# ğŸš€ é«˜çº§RAG Pipeline

> **å­¦ä¹ ç›®æ ‡**ï¼šæ•´åˆæ‰€æœ‰æŠ€æœ¯ï¼Œæ„å»ºç”Ÿäº§çº§RAGç³»ç»Ÿ

---

## 1. å®Œæ•´Pipelineæ¶æ„

```
                    ç”¨æˆ·é—®é¢˜
                        â”‚
                        â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚  æŸ¥è¯¢ç†è§£/æ”¹å†™  â”‚ â† LLM
               â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â–¼                       â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  è¯­ä¹‰æ£€ç´¢     â”‚       â”‚  å…³é”®è¯æ£€ç´¢   â”‚
    â”‚  (Dense)     â”‚       â”‚  (BM25)      â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                       â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚  RRFèåˆæ’åº   â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚  é‡æ’åºRerank  â”‚ â† Cross-Encoder
               â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚  ä¸Šä¸‹æ–‡å‹ç¼©    â”‚ â† LLM/å¥å­æå–
               â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚  ç­”æ¡ˆç”Ÿæˆ      â”‚ â† LLM
               â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â–¼
                    å›ç­”
```

---

## 2. å®Œæ•´å®ç°

```python
import chromadb
from rank_bm25 import BM25Okapi
from sentence_transformers import CrossEncoder, SentenceTransformer
from openai import OpenAI
import jieba
import numpy as np
from typing import Optional
import os

class AdvancedRAG:
    """ç”Ÿäº§çº§RAGç³»ç»Ÿ"""
    
    def __init__(
        self,
        persist_path: str = "./advanced_rag_db",
        semantic_weight: float = 0.6,
        rerank_top_k: int = 20,
        final_top_k: int = 5
    ):
        # å‘é‡æ•°æ®åº“
        self.chroma = chromadb.PersistentClient(path=persist_path)
        self.collection = self.chroma.get_or_create_collection("knowledge")
        
        # BM25ç´¢å¼•
        self.documents = []
        self.bm25 = None
        
        # æ¨¡å‹
        self.reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
        self.sentence_model = SentenceTransformer('BAAI/bge-small-zh-v1.5')
        
        # LLM
        self.llm = OpenAI(
            api_key=os.getenv("DEEPSEEK_API_KEY"),
            base_url="https://api.deepseek.com/v1"
        )
        
        # é…ç½®
        self.semantic_weight = semantic_weight
        self.rerank_top_k = rerank_top_k
        self.final_top_k = final_top_k
    
    # ============ æ–‡æ¡£ç®¡ç† ============
    
    def add_documents(self, documents: list[str], metadatas: list[dict] = None):
        """æ·»åŠ æ–‡æ¡£"""
        ids = [f"doc_{self.collection.count() + i}" for i in range(len(documents))]
        
        # ChromaDB
        self.collection.upsert(
            documents=documents,
            ids=ids,
            metadatas=metadatas
        )
        
        # BM25
        self.documents.extend(documents)
        tokenized = [list(jieba.cut(doc)) for doc in self.documents]
        self.bm25 = BM25Okapi(tokenized)
        
        print(f"æ·»åŠ  {len(documents)} ä¸ªæ–‡æ¡£ï¼Œæ€»æ•°: {self.collection.count()}")
    
    # ============ æŸ¥è¯¢æ”¹å†™ ============
    
    def _rewrite_query(self, query: str) -> list[str]:
        """LLMæŸ¥è¯¢æ”¹å†™"""
        response = self.llm.chat.completions.create(
            model="deepseek-chat",
            messages=[{
                "role": "user",
                "content": f"""å°†ä»¥ä¸‹æŸ¥è¯¢æ”¹å†™æˆ3ä¸ªä¸åŒçš„è¡¨è¾¾æ–¹å¼ï¼Œä¿æŒè¯­ä¹‰ç›¸åŒï¼š

æŸ¥è¯¢ï¼š{query}

ç›´æ¥è¾“å‡º3ä¸ªæ”¹å†™ï¼Œæ¯è¡Œä¸€ä¸ªï¼š"""
            }],
            temperature=0.7,
            max_tokens=200
        )
        
        rewrites = response.choices[0].message.content.strip().split('\n')
        return [query] + [r.strip() for r in rewrites[:3] if r.strip()]
    
    # ============ æ··åˆæ£€ç´¢ ============
    
    def _semantic_search(self, query: str, top_k: int) -> list[tuple[str, float]]:
        """è¯­ä¹‰æ£€ç´¢"""
        results = self.collection.query(
            query_texts=[query],
            n_results=top_k,
            include=["documents", "distances"]
        )
        
        return list(zip(
            results["documents"][0],
            [1 - d for d in results["distances"][0]]  # è½¬ç›¸ä¼¼åº¦
        ))
    
    def _keyword_search(self, query: str, top_k: int) -> list[tuple[str, float]]:
        """å…³é”®è¯æ£€ç´¢"""
        if not self.bm25:
            return []
        
        tokens = list(jieba.cut(query))
        scores = self.bm25.get_scores(tokens)
        
        if scores.max() > 0:
            scores = scores / scores.max()
        
        indices = scores.argsort()[-top_k:][::-1]
        return [(self.documents[i], scores[i]) for i in indices]
    
    def _hybrid_search(self, queries: list[str], top_k: int) -> list[str]:
        """æ··åˆæ£€ç´¢+RRFèåˆ"""
        doc_scores = {}
        
        for query in queries:
            semantic = self._semantic_search(query, top_k)
            keyword = self._keyword_search(query, top_k)
            
            # åŠ æƒèåˆ
            for doc, score in semantic:
                doc_scores[doc] = doc_scores.get(doc, 0) + self.semantic_weight * score
            
            for doc, score in keyword:
                doc_scores[doc] = doc_scores.get(doc, 0) + (1 - self.semantic_weight) * score
        
        # æ’åº
        sorted_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)
        return [doc for doc, _ in sorted_docs[:top_k]]
    
    # ============ é‡æ’åº ============
    
    def _rerank(self, query: str, documents: list[str], top_k: int) -> list[str]:
        """äº¤å‰ç¼–ç å™¨é‡æ’åº"""
        if not documents:
            return []
        
        pairs = [(query, doc) for doc in documents]
        scores = self.reranker.predict(pairs)
        
        ranked = sorted(zip(documents, scores), key=lambda x: x[1], reverse=True)
        return [doc for doc, _ in ranked[:top_k]]
    
    # ============ ä¸Šä¸‹æ–‡å‹ç¼© ============
    
    def _compress_context(self, query: str, documents: list[str]) -> str:
        """å¥å­çº§å‹ç¼©"""
        all_sentences = []
        for doc in documents:
            sentences = [s.strip() for s in doc.replace('\n', 'ã€‚').split('ã€‚') if s.strip()]
            all_sentences.extend(sentences)
        
        if len(all_sentences) <= 5:
            return "\n\n".join(documents)
        
        # æŒ‰ç›¸ä¼¼åº¦é€‰æ‹©å¥å­
        query_emb = self.sentence_model.encode([query])
        sent_embs = self.sentence_model.encode(all_sentences)
        
        similarities = np.dot(sent_embs, query_emb.T).flatten()
        top_indices = similarities.argsort()[-10:][::-1]  # Top 10å¥å­
        
        selected = [all_sentences[i] for i in sorted(top_indices)]
        return 'ã€‚'.join(selected) + 'ã€‚'
    
    # ============ ç­”æ¡ˆç”Ÿæˆ ============
    
    def _generate_answer(self, query: str, context: str) -> str:
        """ç”Ÿæˆå›ç­”"""
        response = self.llm.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": """ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†åº“é—®ç­”åŠ©æ‰‹ã€‚æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚
- åªæ ¹æ®ä¸Šä¸‹æ–‡ä¸­çš„ä¿¡æ¯å›ç­”
- å¦‚æœä¸Šä¸‹æ–‡æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œæ˜ç¡®è¯´æ˜
- å›ç­”è¦ç®€æ´å‡†ç¡®"""},
                {"role": "user", "content": f"ä¸Šä¸‹æ–‡ï¼š\n{context}\n\né—®é¢˜ï¼š{query}"}
            ],
            temperature=0.3
        )
        return response.choices[0].message.content
    
    # ============ ä¸»æŸ¥è¯¢æ–¹æ³• ============
    
    def query(
        self, 
        question: str,
        enable_rewrite: bool = True,
        enable_rerank: bool = True,
        enable_compress: bool = True
    ) -> dict:
        """å®Œæ•´RAGæŸ¥è¯¢"""
        
        # 1. æŸ¥è¯¢æ”¹å†™
        if enable_rewrite:
            queries = self._rewrite_query(question)
        else:
            queries = [question]
        
        # 2. æ··åˆæ£€ç´¢
        candidates = self._hybrid_search(queries, self.rerank_top_k)
        
        if not candidates:
            return {
                "answer": "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚",
                "sources": [],
                "steps": {"rewrite": queries, "candidates": 0}
            }
        
        # 3. é‡æ’åº
        if enable_rerank:
            documents = self._rerank(question, candidates, self.final_top_k)
        else:
            documents = candidates[:self.final_top_k]
        
        # 4. ä¸Šä¸‹æ–‡å‹ç¼©
        if enable_compress:
            context = self._compress_context(question, documents)
        else:
            context = "\n\n".join(documents)
        
        # 5. ç”Ÿæˆå›ç­”
        answer = self._generate_answer(question, context)
        
        return {
            "answer": answer,
            "sources": documents,
            "context_length": len(context),
            "steps": {
                "rewrite_queries": queries,
                "candidates_count": len(candidates),
                "final_docs": len(documents)
            }
        }
    
    def query_stream(self, question: str):
        """æµå¼æŸ¥è¯¢"""
        queries = self._rewrite_query(question)
        candidates = self._hybrid_search(queries, self.rerank_top_k)
        documents = self._rerank(question, candidates, self.final_top_k)
        context = self._compress_context(question, documents)
        
        response = self.llm.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": "æ ¹æ®ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚"},
                {"role": "user", "content": f"ä¸Šä¸‹æ–‡ï¼š\n{context}\n\né—®é¢˜ï¼š{question}"}
            ],
            stream=True
        )
        
        for chunk in response:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content


# ============ ä½¿ç”¨ç¤ºä¾‹ ============

if __name__ == "__main__":
    rag = AdvancedRAG()
    
    # æ·»åŠ çŸ¥è¯†
    rag.add_documents([
        "FastAPIæ˜¯ä¸€ä¸ªç°ä»£ã€å¿«é€Ÿçš„Python Webæ¡†æ¶ã€‚å®ƒåŸºäºStarletteå’ŒPydanticã€‚",
        "FastAPIçš„æ€§èƒ½éå¸¸å‡ºè‰²ï¼Œbenchmarkæµ‹è¯•æ˜¾ç¤ºå…¶QPSå¯è¾¾åˆ°10000+ï¼Œä¸Goå’ŒNode.jsç›¸å½“ã€‚",
        "FastAPIè‡ªåŠ¨ç”ŸæˆOpenAPIæ–‡æ¡£ï¼Œæ”¯æŒSwagger UIå’ŒReDocã€‚",
        "FastAPIä½¿ç”¨Pythonç±»å‹æç¤ºè¿›è¡Œæ•°æ®éªŒè¯ï¼Œå‡å°‘äº†å¤§é‡æ ·æ¿ä»£ç ã€‚",
        "Djangoæ˜¯ä¸€ä¸ªå…¨åŠŸèƒ½çš„Python Webæ¡†æ¶ï¼ŒåŒ…å«ORMã€ç®¡ç†åå°ã€è®¤è¯ç³»ç»Ÿç­‰ã€‚",
        "Flaskæ˜¯ä¸€ä¸ªè½»é‡çº§çš„Pythonå¾®æ¡†æ¶ï¼Œé€‚åˆå°å‹é¡¹ç›®å’ŒAPIå¼€å‘ã€‚"
    ])
    
    # æŸ¥è¯¢
    result = rag.query("FastAPIçš„æ€§èƒ½æ€ä¹ˆæ ·ï¼Ÿå’Œå…¶ä»–æ¡†æ¶ç›¸æ¯”å¦‚ä½•ï¼Ÿ")
    
    print("å›ç­”:", result["answer"])
    print(f"\nä½¿ç”¨äº† {len(result['sources'])} ä¸ªæ–‡æ¡£")
    print(f"ä¸Šä¸‹æ–‡é•¿åº¦: {result['context_length']} å­—ç¬¦")
    print(f"æ­¥éª¤: {result['steps']}")
```

---

## 3. FastAPIæ¥å£

```python
from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from pydantic import BaseModel

app = FastAPI(title="Advanced RAG API")
rag = AdvancedRAG()

class QueryRequest(BaseModel):
    question: str
    enable_rewrite: bool = True
    enable_rerank: bool = True
    enable_compress: bool = True

@app.post("/query")
async def query(request: QueryRequest):
    result = rag.query(
        request.question,
        enable_rewrite=request.enable_rewrite,
        enable_rerank=request.enable_rerank,
        enable_compress=request.enable_compress
    )
    return result

@app.post("/query/stream")
async def query_stream(request: QueryRequest):
    def generate():
        for text in rag.query_stream(request.question):
            yield f"data: {text}\n\n"
    return StreamingResponse(generate(), media_type="text/event-stream")
```

---

## 4. æ€§èƒ½ä¼˜åŒ–

### 4.1 æ‰¹é‡Embedding

```python
# æ‰¹é‡å¤„ç†è€Œéé€ä¸ª
embeddings = model.encode(texts, batch_size=32)
```

### 4.2 æ¨¡å‹ç¼“å­˜

```python
from functools import lru_cache

@lru_cache(maxsize=1)
def get_reranker():
    return CrossEncoder('...')
```

### 4.3 å¼‚æ­¥å¤„ç†

```python
import asyncio

async def async_query(rag, questions):
    tasks = [asyncio.to_thread(rag.query, q) for q in questions]
    return await asyncio.gather(*tasks)
```

---

## ğŸ“º æ¨èBç«™è§†é¢‘

æœç´¢ï¼š
- **"RAG ç”Ÿäº§çº§ æ¶æ„"**
- **"é«˜çº§RAG Pipeline"**
- **"RAG æ€§èƒ½ä¼˜åŒ–"**

---

## 5. ç»§ç»­å­¦ä¹ 

ğŸ‰ **æ­å–œå®ŒæˆWeek 5ï¼**

ğŸ“Œ **Week 5 å­¦ä¹ é¡ºåº**ï¼š
1. âœ… æ··åˆæ£€ç´¢
2. âœ… é‡æ’åºæ¨¡å‹
3. âœ… ä¸Šä¸‹æ–‡å‹ç¼©
4. âœ… é«˜çº§RAG Pipelineï¼ˆæœ¬æ•™ç¨‹ï¼‰

ç»§ç»­å‰å¾€ **Week 6** å­¦ä¹ AI Agentï¼

---

**ä½ å·²æŒæ¡ç”Ÿäº§çº§RAGæŠ€æœ¯ï¼ğŸ’ª**
