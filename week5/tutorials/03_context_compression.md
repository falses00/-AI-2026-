# ğŸ“¦ ä¸Šä¸‹æ–‡å‹ç¼©æŠ€æœ¯

> **å­¦ä¹ ç›®æ ‡**ï¼šæŒæ¡ä¸Šä¸‹æ–‡å‹ç¼©æŠ€æœ¯ï¼Œæå‡RAGæ•ˆç‡å’Œæ•ˆæœ

---

## 1. ä¸ºä»€ä¹ˆéœ€è¦ä¸Šä¸‹æ–‡å‹ç¼©ï¼Ÿ

### é—®é¢˜1ï¼šæ£€ç´¢ç»“æœå¤ªå¤š

```
æ£€ç´¢è¿”å›5ä¸ªæ–‡æ¡£ï¼Œæ¯ä¸ª1000å­— â†’ 5000å­—ä¸Šä¸‹æ–‡
LLMä¸Šä¸‹æ–‡çª—å£æœ‰é™ + è´¹ç”¨é«˜
```

### é—®é¢˜2ï¼šå™ªéŸ³å¤ªå¤š

```
æ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼š
"FastAPIæ˜¯ä¸€ä¸ªç°ä»£Pythonæ¡†æ¶ã€‚å®ƒç”±SebastiÃ¡n RamÃ­rezåˆ›å»ºã€‚
FastAPIæ”¯æŒå¼‚æ­¥ã€‚å®‰è£…å‘½ä»¤æ˜¯pip install fastapiã€‚
FastAPIçš„æ€§èƒ½éå¸¸å¥½ï¼Œæ¥è¿‘Goè¯­è¨€ã€‚"
                    â†‘
ç”¨æˆ·é—®çš„æ˜¯"FastAPIæ€§èƒ½"ï¼Œåªæœ‰æœ€åä¸€å¥æ‰æ˜¯ç­”æ¡ˆ
```

### è§£å†³æ–¹æ¡ˆï¼šä¸Šä¸‹æ–‡å‹ç¼©

**åªä¿ç•™ä¸é—®é¢˜ç›¸å…³çš„å†…å®¹ï¼**

---

## 2. å‹ç¼©æ–¹æ³•

### 2.1 LLMæå–ï¼ˆæœ€ç²¾å‡†ï¼‰

è®©LLMæå–ç›¸å…³ä¿¡æ¯ï¼š

```python
from openai import OpenAI

def compress_with_llm(query: str, document: str, client: OpenAI) -> str:
    """ä½¿ç”¨LLMå‹ç¼©æ–‡æ¡£"""
    response = client.chat.completions.create(
        model="deepseek-chat",
        messages=[{
            "role": "user",
            "content": f"""ä»ä»¥ä¸‹æ–‡æ¡£ä¸­æå–ä¸é—®é¢˜ç›¸å…³çš„ä¿¡æ¯ã€‚åªè¾“å‡ºç›¸å…³å†…å®¹ï¼Œä¸è¦è§£é‡Šã€‚
å¦‚æœæ²¡æœ‰ç›¸å…³å†…å®¹ï¼Œè¾“å‡º"æ— ç›¸å…³å†…å®¹"ã€‚

é—®é¢˜ï¼š{query}

æ–‡æ¡£ï¼š
{document}

ç›¸å…³å†…å®¹ï¼š"""
        }],
        temperature=0,
        max_tokens=500
    )
    return response.choices[0].message.content

# ä½¿ç”¨
doc = """FastAPIæ˜¯ä¸€ä¸ªç°ä»£Pythonæ¡†æ¶ã€‚å®ƒç”±SebastiÃ¡n RamÃ­rezåˆ›å»ºã€‚
FastAPIæ”¯æŒå¼‚æ­¥ã€‚å®‰è£…å‘½ä»¤æ˜¯pip install fastapiã€‚
FastAPIçš„æ€§èƒ½éå¸¸å¥½ï¼Œæ¥è¿‘Goè¯­è¨€ï¼ŒQPSå¯è¾¾10000+ã€‚"""

compressed = compress_with_llm("FastAPIæ€§èƒ½å¦‚ä½•?", doc, client)
print(compressed)
# è¾“å‡º: FastAPIçš„æ€§èƒ½éå¸¸å¥½ï¼Œæ¥è¿‘Goè¯­è¨€ï¼ŒQPSå¯è¾¾10000+ã€‚
```

### 2.2 åŸºäºå¥å­çš„æå–

```python
import numpy as np
from sentence_transformers import SentenceTransformer

def sentence_extraction(query: str, document: str, top_k: int = 3) -> str:
    """æå–æœ€ç›¸å…³çš„å¥å­"""
    model = SentenceTransformer('BAAI/bge-small-zh-v1.5')
    
    # åˆ†å¥
    sentences = [s.strip() for s in document.split('ã€‚') if s.strip()]
    
    # ç¼–ç 
    query_emb = model.encode([query])
    sent_embs = model.encode(sentences)
    
    # è®¡ç®—ç›¸ä¼¼åº¦
    similarities = np.dot(sent_embs, query_emb.T).flatten()
    
    # è·å–Top-Kå¥å­
    top_indices = similarities.argsort()[-top_k:][::-1]
    top_sentences = [sentences[i] for i in sorted(top_indices)]
    
    return 'ã€‚'.join(top_sentences) + 'ã€‚'

# ä½¿ç”¨
compressed = sentence_extraction("FastAPIæ€§èƒ½", doc, top_k=2)
```

### 2.3 å…³é”®è¯è¿‡æ»¤

```python
import jieba

def keyword_filter(query: str, document: str, threshold: float = 0.3) -> str:
    """ä¿ç•™åŒ…å«æŸ¥è¯¢å…³é”®è¯çš„å¥å­"""
    query_words = set(jieba.cut(query))
    sentences = [s.strip() for s in document.split('ã€‚') if s.strip()]
    
    relevant = []
    for sent in sentences:
        sent_words = set(jieba.cut(sent))
        overlap = len(query_words & sent_words) / len(query_words)
        if overlap >= threshold:
            relevant.append(sent)
    
    return 'ã€‚'.join(relevant) + 'ã€‚' if relevant else document
```

---

## 3. å®Œæ•´å‹ç¼©å™¨ç±»

```python
from openai import OpenAI
from sentence_transformers import SentenceTransformer
import numpy as np
from enum import Enum

class CompressionMethod(Enum):
    LLM = "llm"
    SENTENCE = "sentence"
    KEYWORD = "keyword"

class ContextCompressor:
    def __init__(
        self, 
        method: CompressionMethod = CompressionMethod.SENTENCE,
        llm_client: OpenAI = None
    ):
        self.method = method
        self.llm_client = llm_client
        
        if method == CompressionMethod.SENTENCE:
            self.encoder = SentenceTransformer('BAAI/bge-small-zh-v1.5')
    
    def compress(
        self, 
        query: str, 
        documents: list[str],
        max_length: int = 2000
    ) -> str:
        """å‹ç¼©å¤šä¸ªæ–‡æ¡£"""
        compressed_docs = []
        
        for doc in documents:
            if self.method == CompressionMethod.LLM:
                compressed = self._compress_llm(query, doc)
            elif self.method == CompressionMethod.SENTENCE:
                compressed = self._compress_sentence(query, doc)
            else:
                compressed = self._compress_keyword(query, doc)
            
            if compressed and compressed != "æ— ç›¸å…³å†…å®¹":
                compressed_docs.append(compressed)
        
        # åˆå¹¶å¹¶æˆªæ–­
        result = "\n\n".join(compressed_docs)
        if len(result) > max_length:
            result = result[:max_length] + "..."
        
        return result
    
    def _compress_llm(self, query: str, document: str) -> str:
        if not self.llm_client:
            raise ValueError("LLMå‹ç¼©éœ€è¦æä¾›llm_client")
        
        response = self.llm_client.chat.completions.create(
            model="deepseek-chat",
            messages=[{
                "role": "user",
                "content": f"""æå–ä¸é—®é¢˜ç›¸å…³çš„ä¿¡æ¯ï¼š

é—®é¢˜ï¼š{query}
æ–‡æ¡£ï¼š{document}

ç›¸å…³å†…å®¹ï¼ˆå¦‚æœæ²¡æœ‰åˆ™è¾“å‡º"æ— ç›¸å…³å†…å®¹"ï¼‰ï¼š"""
            }],
            temperature=0,
            max_tokens=500
        )
        return response.choices[0].message.content
    
    def _compress_sentence(self, query: str, document: str, top_k: int = 3) -> str:
        sentences = [s.strip() for s in document.replace('\n', 'ã€‚').split('ã€‚') if s.strip()]
        
        if len(sentences) <= top_k:
            return document
        
        query_emb = self.encoder.encode([query])
        sent_embs = self.encoder.encode(sentences)
        
        similarities = np.dot(sent_embs, query_emb.T).flatten()
        top_indices = similarities.argsort()[-top_k:][::-1]
        top_sentences = [sentences[i] for i in sorted(top_indices)]
        
        return 'ã€‚'.join(top_sentences) + 'ã€‚'
    
    def _compress_keyword(self, query: str, document: str) -> str:
        import jieba
        query_words = set(jieba.cut(query))
        sentences = [s.strip() for s in document.split('ã€‚') if s.strip()]
        
        relevant = [s for s in sentences 
                   if any(w in s for w in query_words if len(w) > 1)]
        
        return 'ã€‚'.join(relevant) + 'ã€‚' if relevant else ""

# ä½¿ç”¨
compressor = ContextCompressor(method=CompressionMethod.SENTENCE)
compressed = compressor.compress(
    query="FastAPIæ€§èƒ½",
    documents=[doc1, doc2, doc3]
)
```

---

## 4. é›†æˆåˆ°RAG

```python
import chromadb
from openai import OpenAI

class RAGWithCompression:
    def __init__(self):
        self.chroma = chromadb.PersistentClient(path="./compress_db")
        self.collection = self.chroma.get_or_create_collection("docs")
        self.llm = OpenAI(api_key="key", base_url="https://api.deepseek.com/v1")
        self.compressor = ContextCompressor(
            method=CompressionMethod.LLM,
            llm_client=self.llm
        )
    
    def add_documents(self, docs: list[str]):
        ids = [f"doc_{i}" for i in range(len(docs))]
        self.collection.upsert(documents=docs, ids=ids)
    
    def query(self, question: str) -> str:
        # 1. æ£€ç´¢
        results = self.collection.query(
            query_texts=[question],
            n_results=10
        )
        documents = results["documents"][0]
        
        # 2. å‹ç¼©
        compressed_context = self.compressor.compress(
            query=question,
            documents=documents,
            max_length=2000
        )
        
        # 3. ç”Ÿæˆå›ç­”
        response = self.llm.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": "æ ¹æ®æä¾›çš„ä¿¡æ¯å›ç­”é—®é¢˜ã€‚"},
                {"role": "user", "content": f"ä¿¡æ¯ï¼š{compressed_context}\n\né—®é¢˜ï¼š{question}"}
            ]
        )
        
        return response.choices[0].message.content

# ä½¿ç”¨
rag = RAGWithCompression()
rag.add_documents([...])
answer = rag.query("FastAPIçš„ä¸»è¦ä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ")
```

---

## 5. LangChainä¸Šä¸‹æ–‡å‹ç¼©

```python
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor
from langchain_openai import ChatOpenAI

# åˆ›å»ºLLM
llm = ChatOpenAI(
    model="deepseek-chat",
    openai_api_key="key",
    openai_api_base="https://api.deepseek.com/v1"
)

# åˆ›å»ºå‹ç¼©å™¨
compressor = LLMChainExtractor.from_llm(llm)

# åˆ›å»ºå‹ç¼©æ£€ç´¢å™¨
compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=vector_retriever  # ä½ çš„åŸºç¡€æ£€ç´¢å™¨
)

# ä½¿ç”¨
docs = compression_retriever.invoke("FastAPIæ€§èƒ½")
for doc in docs:
    print(doc.page_content)  # å·²å‹ç¼©çš„å†…å®¹
```

---

## 6. å‹ç¼©æ•ˆæœå¯¹æ¯”

| æ–¹æ³• | å‹ç¼©ç‡ | ç²¾åº¦ | é€Ÿåº¦ | æˆæœ¬ |
|------|--------|------|------|------|
| LLM | é«˜ | æœ€é«˜ | æ…¢ | é«˜ |
| Sentence | ä¸­ | é«˜ | å¿« | æ—  |
| Keyword | ä½ | ä¸­ | æœ€å¿« | æ—  |

**å»ºè®®**ï¼š
- å¼€å‘æµ‹è¯•ï¼šSentence
- ç”Ÿäº§ç¯å¢ƒï¼šLLMï¼ˆç²¾åº¦è¦æ±‚é«˜ï¼‰æˆ–Sentenceï¼ˆæˆæœ¬æ•æ„Ÿï¼‰

---

## ğŸ“º æ¨èBç«™è§†é¢‘

æœç´¢ï¼š
- **"RAG ä¸Šä¸‹æ–‡å‹ç¼©"**
- **"LangChain Compression"**
- **"Context Window ä¼˜åŒ–"**

---

## 7. ç»§ç»­å­¦ä¹ 

ğŸ“Œ **Week 5 å­¦ä¹ é¡ºåº**ï¼š
1. âœ… æ··åˆæ£€ç´¢
2. âœ… é‡æ’åºæ¨¡å‹
3. âœ… ä¸Šä¸‹æ–‡å‹ç¼©æŠ€æœ¯ï¼ˆæœ¬æ•™ç¨‹ï¼‰
4. â¡ï¸ é«˜çº§RAG Pipeline

---

**å‹ç¼©è®©RAGæ›´é«˜æ•ˆï¼ğŸ’ª**
