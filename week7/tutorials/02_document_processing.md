# ğŸ“„ å¤šæ ¼å¼æ–‡æ¡£å¤„ç†Pipeline

> **å­¦ä¹ ç›®æ ‡**ï¼šæ„å»ºæ”¯æŒPDFã€Wordã€ç½‘é¡µçš„ç»Ÿä¸€æ–‡æ¡£å¤„ç†ç³»ç»Ÿ

---

## 1. æ–‡æ¡£å¤„ç†æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æ–‡æ¡£å¤„ç†Pipelineæ¶æ„                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚  PDF    â”‚  â”‚  Word   â”‚  â”‚  HTML   â”‚  â”‚  TXT    â”‚            â”‚
â”‚  â”‚ æ–‡æ¡£    â”‚  â”‚ æ–‡æ¡£    â”‚  â”‚ ç½‘é¡µ    â”‚  â”‚ æ–‡æœ¬    â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜            â”‚
â”‚       â”‚            â”‚            â”‚            â”‚                  â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                         â”‚                                        â”‚
â”‚                         â–¼                                        â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚              â”‚   DocumentLoader     â”‚                            â”‚
â”‚              â”‚   (ç»Ÿä¸€åŠ è½½æ¥å£)      â”‚                            â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â”‚                         â”‚                                        â”‚
â”‚                         â–¼                                        â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚              â”‚   TextSplitter       â”‚                            â”‚
â”‚              â”‚   (æ™ºèƒ½åˆ†å—)          â”‚                            â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â”‚                         â”‚                                        â”‚
â”‚                         â–¼                                        â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚              â”‚   Embedder           â”‚                            â”‚
â”‚              â”‚   (å‘é‡åŒ–)            â”‚                            â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â”‚                         â”‚                                        â”‚
â”‚                         â–¼                                        â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚              â”‚   VectorStore        â”‚                            â”‚
â”‚              â”‚   (ChromaDB/Milvus)  â”‚                            â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. PDFæ–‡æ¡£å¤„ç†

### 2.1 å®‰è£…ä¾èµ–

```bash
pip install pdfplumber pymupdf pypdf2
```

### 2.2 PDFè§£æå™¨

```python
import pdfplumber
from dataclasses import dataclass
from typing import Optional

@dataclass
class PDFPage:
    """PDFé¡µé¢æ•°æ®"""
    page_num: int
    text: str
    tables: list[list[list[str]]]
    images: list[bytes]

@dataclass 
class PDFDocument:
    """PDFæ–‡æ¡£æ•°æ®"""
    filename: str
    pages: list[PDFPage]
    metadata: dict

class PDFParser:
    """PDFè§£æå™¨"""
    
    def parse(self, file_path: str) -> PDFDocument:
        """è§£æPDFæ–‡ä»¶"""
        pages = []
        
        with pdfplumber.open(file_path) as pdf:
            metadata = pdf.metadata or {}
            
            for i, page in enumerate(pdf.pages):
                # æå–æ–‡æœ¬
                text = page.extract_text() or ""
                
                # æå–è¡¨æ ¼
                tables = page.extract_tables() or []
                
                # æå–å›¾ç‰‡ï¼ˆç®€åŒ–å¤„ç†ï¼‰
                images = []
                
                pages.append(PDFPage(
                    page_num=i + 1,
                    text=text,
                    tables=tables,
                    images=images
                ))
        
        return PDFDocument(
            filename=file_path,
            pages=pages,
            metadata=metadata
        )
    
    def extract_text(self, file_path: str) -> str:
        """åªæå–æ–‡æœ¬"""
        doc = self.parse(file_path)
        return "\n\n".join(page.text for page in doc.pages)

# ä½¿ç”¨
parser = PDFParser()
doc = parser.parse("example.pdf")
print(f"å…± {len(doc.pages)} é¡µ")
print(f"ç¬¬1é¡µæ–‡æœ¬: {doc.pages[0].text[:200]}...")
```

---

## 3. Wordæ–‡æ¡£å¤„ç†

### 3.1 å®‰è£…ä¾èµ–

```bash
pip install python-docx
```

### 3.2 Wordè§£æå™¨

```python
from docx import Document
from docx.table import Table
from dataclasses import dataclass

@dataclass
class WordDocument:
    """Wordæ–‡æ¡£æ•°æ®"""
    filename: str
    paragraphs: list[str]
    tables: list[list[list[str]]]
    metadata: dict

class WordParser:
    """Wordè§£æå™¨"""
    
    def parse(self, file_path: str) -> WordDocument:
        """è§£æWordæ–‡ä»¶"""
        doc = Document(file_path)
        
        # æå–æ®µè½
        paragraphs = [p.text for p in doc.paragraphs if p.text.strip()]
        
        # æå–è¡¨æ ¼
        tables = []
        for table in doc.tables:
            table_data = []
            for row in table.rows:
                row_data = [cell.text for cell in row.cells]
                table_data.append(row_data)
            tables.append(table_data)
        
        # æå–å…ƒæ•°æ®
        metadata = {
            "author": doc.core_properties.author,
            "title": doc.core_properties.title,
            "created": str(doc.core_properties.created),
        }
        
        return WordDocument(
            filename=file_path,
            paragraphs=paragraphs,
            tables=tables,
            metadata=metadata
        )
    
    def extract_text(self, file_path: str) -> str:
        """åªæå–æ–‡æœ¬"""
        doc = self.parse(file_path)
        return "\n\n".join(doc.paragraphs)

# ä½¿ç”¨
parser = WordParser()
doc = parser.parse("example.docx")
print(f"å…± {len(doc.paragraphs)} ä¸ªæ®µè½")
```

---

## 4. ç½‘é¡µå†…å®¹å¤„ç†

### 4.1 å®‰è£…ä¾èµ–

```bash
pip install beautifulsoup4 httpx trafilatura
```

### 4.2 ç½‘é¡µè§£æå™¨

```python
import httpx
from bs4 import BeautifulSoup
import trafilatura
from dataclasses import dataclass

@dataclass
class WebPage:
    """ç½‘é¡µæ•°æ®"""
    url: str
    title: str
    text: str
    links: list[str]
    metadata: dict

class WebParser:
    """ç½‘é¡µè§£æå™¨"""
    
    async def fetch(self, url: str) -> str:
        """è·å–ç½‘é¡µå†…å®¹"""
        async with httpx.AsyncClient() as client:
            response = await client.get(url, follow_redirects=True)
            response.raise_for_status()
            return response.text
    
    def parse(self, html: str, url: str) -> WebPage:
        """è§£æHTML"""
        soup = BeautifulSoup(html, 'html.parser')
        
        # æå–æ ‡é¢˜
        title = soup.title.string if soup.title else ""
        
        # ä½¿ç”¨trafilaturaæå–æ­£æ–‡ï¼ˆè¿‡æ»¤å¹¿å‘Šã€å¯¼èˆªç­‰ï¼‰
        text = trafilatura.extract(html) or ""
        
        # æå–é“¾æ¥
        links = []
        for a in soup.find_all('a', href=True):
            href = a['href']
            if href.startswith('http'):
                links.append(href)
        
        return WebPage(
            url=url,
            title=title,
            text=text,
            links=links[:20],  # åªä¿ç•™å‰20ä¸ªé“¾æ¥
            metadata={"source": "web"}
        )
    
    async def parse_url(self, url: str) -> WebPage:
        """ä»URLè§£æç½‘é¡µ"""
        html = await self.fetch(url)
        return self.parse(html, url)

# ä½¿ç”¨
import asyncio

async def main():
    parser = WebParser()
    page = await parser.parse_url("https://example.com")
    print(f"æ ‡é¢˜: {page.title}")
    print(f"æ­£æ–‡: {page.text[:500]}...")

asyncio.run(main())
```

---

## 5. ç»Ÿä¸€æ–‡æ¡£åŠ è½½å™¨

```python
from pathlib import Path
from typing import Union
from enum import Enum

class DocumentType(Enum):
    PDF = "pdf"
    WORD = "docx"
    TEXT = "txt"
    HTML = "html"
    WEB = "web"

@dataclass
class Document:
    """ç»Ÿä¸€æ–‡æ¡£æ ¼å¼"""
    source: str
    doc_type: DocumentType
    content: str
    metadata: dict
    chunks: list[str] = None

class UnifiedDocumentLoader:
    """ç»Ÿä¸€æ–‡æ¡£åŠ è½½å™¨"""
    
    def __init__(self):
        self.pdf_parser = PDFParser()
        self.word_parser = WordParser()
        self.web_parser = WebParser()
    
    def load(self, source: str) -> Document:
        """åŠ è½½æ–‡æ¡£"""
        # åˆ¤æ–­ç±»å‹
        if source.startswith("http"):
            return self._load_web(source)
        
        path = Path(source)
        suffix = path.suffix.lower()
        
        if suffix == ".pdf":
            return self._load_pdf(source)
        elif suffix in [".docx", ".doc"]:
            return self._load_word(source)
        elif suffix == ".txt":
            return self._load_text(source)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {suffix}")
    
    def _load_pdf(self, path: str) -> Document:
        text = self.pdf_parser.extract_text(path)
        return Document(
            source=path,
            doc_type=DocumentType.PDF,
            content=text,
            metadata={"type": "pdf"}
        )
    
    def _load_word(self, path: str) -> Document:
        text = self.word_parser.extract_text(path)
        return Document(
            source=path,
            doc_type=DocumentType.WORD,
            content=text,
            metadata={"type": "word"}
        )
    
    def _load_text(self, path: str) -> Document:
        with open(path, 'r', encoding='utf-8') as f:
            text = f.read()
        return Document(
            source=path,
            doc_type=DocumentType.TEXT,
            content=text,
            metadata={"type": "text"}
        )
    
    async def _load_web(self, url: str) -> Document:
        page = await self.web_parser.parse_url(url)
        return Document(
            source=url,
            doc_type=DocumentType.WEB,
            content=page.text,
            metadata={"type": "web", "title": page.title}
        )

# ä½¿ç”¨
loader = UnifiedDocumentLoader()
doc = loader.load("report.pdf")
print(f"å†…å®¹é•¿åº¦: {len(doc.content)} å­—ç¬¦")
```

---

## 6. æ™ºèƒ½æ–‡æœ¬åˆ†å—

```python
from typing import List

class SmartTextSplitter:
    """æ™ºèƒ½æ–‡æœ¬åˆ†å—å™¨"""
    
    def __init__(
        self,
        chunk_size: int = 1000,
        chunk_overlap: int = 200,
        separators: list[str] = None
    ):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.separators = separators or ["\n\n", "\n", "ã€‚", ".", " "]
    
    def split(self, text: str) -> list[str]:
        """åˆ†å‰²æ–‡æœ¬"""
        chunks = []
        current_chunk = ""
        
        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = text.split("\n\n")
        
        for para in paragraphs:
            if len(current_chunk) + len(para) <= self.chunk_size:
                current_chunk += para + "\n\n"
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = para + "\n\n"
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        # å¤„ç†é‡å 
        if self.chunk_overlap > 0 and len(chunks) > 1:
            overlapped_chunks = []
            for i, chunk in enumerate(chunks):
                if i > 0:
                    # æ·»åŠ å‰ä¸€ä¸ªchunkçš„æœ«å°¾ä½œä¸ºä¸Šä¸‹æ–‡
                    prev_tail = chunks[i-1][-self.chunk_overlap:]
                    chunk = prev_tail + "\n...\n" + chunk
                overlapped_chunks.append(chunk)
            return overlapped_chunks
        
        return chunks

# ä½¿ç”¨
splitter = SmartTextSplitter(chunk_size=500, chunk_overlap=100)
chunks = splitter.split(long_text)
print(f"åˆ†æˆ {len(chunks)} ä¸ªå—")
```

---

## 7. å­¦ä¹ æ£€æŸ¥æ¸…å•

- [ ] èƒ½å¤Ÿè§£æPDFæ–‡æ¡£å¹¶æå–è¡¨æ ¼
- [ ] èƒ½å¤Ÿè§£æWordæ–‡æ¡£
- [ ] èƒ½å¤Ÿçˆ¬å–ç½‘é¡µå¹¶æå–æ­£æ–‡
- [ ] ç†è§£æ–‡æœ¬åˆ†å—ç­–ç•¥

---

## ç»§ç»­å­¦ä¹ 

ğŸ“Œ **Week 7 å­¦ä¹ é¡ºåº**ï¼š
1. âœ… ä¼ä¸šçº§ç³»ç»Ÿæ¶æ„
2. âœ… å¤šæ ¼å¼æ–‡æ¡£å¤„ç†ï¼ˆæœ¬æ•™ç¨‹ï¼‰
3. â¡ï¸ ç”¨æˆ·è®¤è¯ä¸æƒé™
4. â¡ï¸ äº‘å¹³å°éƒ¨ç½²
