# ğŸ¯ è¡Œä¸šä¸“å±æ¨¡å‹å¾®è°ƒé¡¹ç›®

> **Week 9 ç»¼åˆå®æˆ˜é¡¹ç›®** - æ„å»ºé¢†åŸŸä¸“å±çš„å¾®è°ƒæ¨¡å‹

---

## ğŸ¯ é¡¹ç›®ç›®æ ‡

æ„å»ºä¸€ä¸ªå®Œæ•´çš„æ¨¡å‹å¾®è°ƒPipelineï¼Œä»æ•°æ®å‡†å¤‡åˆ°æ¨¡å‹éƒ¨ç½²ï¼š
- æ”¶é›†å’Œæ¸…æ´—è¡Œä¸šæ•°æ®
- ä½¿ç”¨LoRAè¿›è¡Œé«˜æ•ˆå¾®è°ƒ
- è¯„ä¼°æ¨¡å‹æ•ˆæœ
- éƒ¨ç½²å¾®è°ƒåçš„æ¨¡å‹

---

## ğŸ“Š é¡¹ç›®æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     æ¨¡å‹å¾®è°ƒPipelineæ¶æ„                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚   æ•°æ®æ”¶é›†   â”‚ â”€â”€â”€â–º â”‚  æ•°æ®æ¸…æ´—   â”‚ â”€â”€â”€â–º â”‚  æ ¼å¼è½¬æ¢   â”‚      â”‚
â”‚  â”‚  (æ—¥å¿—/æ ‡æ³¨) â”‚      â”‚  (è´¨é‡è¿‡æ»¤) â”‚      â”‚  (JSONL)   â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                   â”‚              â”‚
â”‚                                                   â–¼              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚   æ¨¡å‹éƒ¨ç½²   â”‚ â—„â”€â”€â”€ â”‚  æ¨¡å‹è¯„ä¼°   â”‚ â—„â”€â”€â”€ â”‚  LoRAå¾®è°ƒ   â”‚      â”‚
â”‚  â”‚  (vLLM)     â”‚      â”‚  (è‡ªåŠ¨+äººå·¥) â”‚      â”‚  (Unsloth) â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ é¡¹ç›®ç»“æ„

```
week9/projects/domain_finetuning/
â”œâ”€â”€ README.md              # æœ¬æ–‡ä»¶
â”œâ”€â”€ requirements.txt       # ä¾èµ–
â”œâ”€â”€ config.yaml           # è®­ç»ƒé…ç½®
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/              # åŸå§‹æ•°æ®
â”‚   â”œâ”€â”€ processed/        # å¤„ç†åæ•°æ®
â”‚   â””â”€â”€ splits/           # è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ prepare_data.py   # æ•°æ®å‡†å¤‡è„šæœ¬
â”‚   â”œâ”€â”€ train.py          # è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ evaluate.py       # è¯„ä¼°è„šæœ¬
â”‚   â””â”€â”€ export.py         # å¯¼å‡ºè„šæœ¬
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_processor.py # æ•°æ®å¤„ç†æ¨¡å—
â”‚   â”œâ”€â”€ trainer.py        # è®­ç»ƒæ¨¡å—
â”‚   â””â”€â”€ evaluator.py      # è¯„ä¼°æ¨¡å—
â””â”€â”€ serving/
    â”œâ”€â”€ app.py            # FastAPIæœåŠ¡
    â””â”€â”€ docker-compose.yml
```

---

## ğŸ”§ æ ¸å¿ƒä»£ç 

### 1. æ•°æ®å¤„ç†å™¨ (`src/data_processor.py`)

```python
import json
from pathlib import Path
from dataclasses import dataclass
from typing import List, Optional
import random

@dataclass
class Sample:
    """è®­ç»ƒæ ·æœ¬"""
    instruction: str
    input_text: str
    output: str
    metadata: dict = None

class DataProcessor:
    """æ•°æ®å¤„ç†å™¨"""
    
    def __init__(self, system_prompt: str = None):
        self.system_prompt = system_prompt or "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¡Œä¸šåŠ©æ‰‹ã€‚"
        self.samples: List[Sample] = []
    
    def add_sample(self, instruction: str, input_text: str, output: str):
        """æ·»åŠ æ ·æœ¬"""
        self.samples.append(Sample(
            instruction=instruction,
            input_text=input_text,
            output=output
        ))
    
    def load_from_jsonl(self, file_path: str):
        """ä»JSONLåŠ è½½"""
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                data = json.loads(line)
                self.add_sample(
                    instruction=data.get('instruction', ''),
                    input_text=data.get('input', ''),
                    output=data.get('output', '')
                )
    
    def filter_quality(self, min_output_len: int = 10, max_output_len: int = 2000):
        """è´¨é‡è¿‡æ»¤"""
        filtered = []
        for sample in self.samples:
            # æ£€æŸ¥è¾“å‡ºé•¿åº¦
            if len(sample.output) < min_output_len:
                continue
            if len(sample.output) > max_output_len:
                continue
            # æ£€æŸ¥æ˜¯å¦æœ‰ç©ºå†…å®¹
            if not sample.output.strip():
                continue
            filtered.append(sample)
        
        original = len(self.samples)
        self.samples = filtered
        print(f"è´¨é‡è¿‡æ»¤: {original} -> {len(self.samples)} æ ·æœ¬")
    
    def to_chat_format(self) -> List[dict]:
        """è½¬æ¢ä¸ºèŠå¤©æ ¼å¼"""
        formatted = []
        for sample in self.samples:
            user_content = sample.instruction
            if sample.input_text:
                user_content += f"\n\n{sample.input_text}"
            
            formatted.append({
                "messages": [
                    {"role": "system", "content": self.system_prompt},
                    {"role": "user", "content": user_content},
                    {"role": "assistant", "content": sample.output}
                ]
            })
        return formatted
    
    def split_data(self, train_ratio: float = 0.8, val_ratio: float = 0.1):
        """åˆ†å‰²æ•°æ®é›†"""
        random.shuffle(self.samples)
        n = len(self.samples)
        train_end = int(n * train_ratio)
        val_end = int(n * (train_ratio + val_ratio))
        
        return {
            "train": self.samples[:train_end],
            "val": self.samples[train_end:val_end],
            "test": self.samples[val_end:]
        }
    
    def export_jsonl(self, samples: List[Sample], output_path: str):
        """å¯¼å‡ºJSONL"""
        formatted = []
        for sample in samples:
            user_content = sample.instruction
            if sample.input_text:
                user_content += f"\n\n{sample.input_text}"
            formatted.append({
                "messages": [
                    {"role": "system", "content": self.system_prompt},
                    {"role": "user", "content": user_content},
                    {"role": "assistant", "content": sample.output}
                ]
            })
        
        with open(output_path, 'w', encoding='utf-8') as f:
            for item in formatted:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')
```

### 2. LoRAè®­ç»ƒè„šæœ¬ (`scripts/train.py`)

```python
"""
ä½¿ç”¨Unslothè¿›è¡ŒLoRAå¾®è°ƒ
æ”¯æŒQwen2.5ã€Llama3ç­‰æ¨¡å‹
"""
from unsloth import FastLanguageModel
from trl import SFTTrainer
from transformers import TrainingArguments
from datasets import load_dataset
import yaml

# åŠ è½½é…ç½®
with open("config.yaml", "r") as f:
    config = yaml.safe_load(f)

# æ¨¡å‹é…ç½®
model_name = config.get("model_name", "Qwen/Qwen2.5-7B")
max_seq_length = config.get("max_seq_length", 2048)
lora_r = config.get("lora_r", 16)
lora_alpha = config.get("lora_alpha", 32)

# åŠ è½½æ¨¡å‹
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=model_name,
    max_seq_length=max_seq_length,
    load_in_4bit=True,  # 4bité‡åŒ–èŠ‚çœæ˜¾å­˜
)

# æ·»åŠ LoRAé€‚é…å™¨
model = FastLanguageModel.get_peft_model(
    model,
    r=lora_r,
    lora_alpha=lora_alpha,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                    "gate_proj", "up_proj", "down_proj"],
    lora_dropout=0.05,
    bias="none",
    use_gradient_checkpointing="unsloth",
)

# åŠ è½½æ•°æ®
dataset = load_dataset("json", data_files={
    "train": "data/splits/train.jsonl",
    "validation": "data/splits/val.jsonl"
})

def formatting_func(examples):
    """æ ¼å¼åŒ–å‡½æ•°"""
    return tokenizer.apply_chat_template(
        examples["messages"],
        tokenize=False,
        add_generation_prompt=False
    )

# è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir="./outputs",
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,
    num_train_epochs=3,
    learning_rate=2e-4,
    warmup_ratio=0.1,
    logging_steps=10,
    save_steps=100,
    evaluation_strategy="steps",
    eval_steps=100,
    fp16=True,
    report_to="none",
)

# è®­ç»ƒå™¨
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    formatting_func=formatting_func,
    args=training_args,
    max_seq_length=max_seq_length,
)

# å¼€å§‹è®­ç»ƒ
trainer.train()

# ä¿å­˜æ¨¡å‹
model.save_pretrained("./outputs/final_model")
tokenizer.save_pretrained("./outputs/final_model")

# åˆå¹¶LoRAæƒé‡ï¼ˆå¯é€‰ï¼‰
model.save_pretrained_merged(
    "./outputs/merged_model",
    tokenizer,
    save_method="merged_16bit",
)

print("è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åˆ° ./outputs/")
```

### 3. è¯„ä¼°è„šæœ¬ (`scripts/evaluate.py`)

```python
"""
æ¨¡å‹è¯„ä¼°è„šæœ¬
æ”¯æŒè‡ªåŠ¨åŒ–è¯„ä¼°å’ŒLLM-as-Judge
"""
import json
from openai import OpenAI
from datasets import load_dataset
from tqdm import tqdm

# é…ç½®
client = OpenAI()
EVAL_MODEL = "gpt-4o"

def load_test_data(path: str):
    """åŠ è½½æµ‹è¯•æ•°æ®"""
    with open(path, 'r', encoding='utf-8') as f:
        return [json.loads(line) for line in f]

def generate_response(model, tokenizer, prompt: str) -> str:
    """ä½¿ç”¨å¾®è°ƒæ¨¡å‹ç”Ÿæˆå“åº”"""
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    outputs = model.generate(
        **inputs,
        max_new_tokens=512,
        temperature=0.7,
        do_sample=True
    )
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

def llm_evaluate(question: str, reference: str, candidate: str) -> dict:
    """ä½¿ç”¨LLMè¯„ä¼°"""
    prompt = f"""è¯·è¯„ä¼°AIåŠ©æ‰‹çš„å›ç­”è´¨é‡ã€‚

é—®é¢˜: {question}

å‚è€ƒç­”æ¡ˆ: {reference}

å¾…è¯„ä¼°å›ç­”: {candidate}

è¯·ä»ä»¥ä¸‹ç»´åº¦è¯„åˆ†(1-5)ï¼š
1. å‡†ç¡®æ€§ - ä¿¡æ¯æ˜¯å¦æ­£ç¡®
2. å®Œæ•´æ€§ - æ˜¯å¦å®Œæ•´å›ç­”é—®é¢˜
3. æµç•…æ€§ - è¯­è¨€æ˜¯å¦é€šé¡º
4. ä¸“ä¸šæ€§ - æ˜¯å¦ä½“ç°é¢†åŸŸçŸ¥è¯†

è¿”å›JSONæ ¼å¼ï¼š
{{"accuracy": X, "completeness": X, "fluency": X, "professionalism": X, "overall": X, "reasoning": "..."}}
"""
    
    response = client.chat.completions.create(
        model=EVAL_MODEL,
        messages=[{"role": "user", "content": prompt}],
        response_format={"type": "json_object"}
    )
    
    return json.loads(response.choices[0].message.content)

def run_evaluation(test_path: str, model, tokenizer):
    """è¿è¡Œå®Œæ•´è¯„ä¼°"""
    test_data = load_test_data(test_path)
    results = []
    
    for item in tqdm(test_data, desc="è¯„ä¼°ä¸­"):
        messages = item["messages"]
        question = messages[1]["content"]  # user message
        reference = messages[2]["content"]  # assistant message
        
        # ç”Ÿæˆå“åº”
        candidate = generate_response(model, tokenizer, question)
        
        # LLMè¯„ä¼°
        scores = llm_evaluate(question, reference, candidate)
        
        results.append({
            "question": question,
            "reference": reference,
            "candidate": candidate,
            "scores": scores
        })
    
    # è®¡ç®—å¹³å‡åˆ†
    avg_scores = {
        "accuracy": sum(r["scores"]["accuracy"] for r in results) / len(results),
        "completeness": sum(r["scores"]["completeness"] for r in results) / len(results),
        "fluency": sum(r["scores"]["fluency"] for r in results) / len(results),
        "professionalism": sum(r["scores"]["professionalism"] for r in results) / len(results),
        "overall": sum(r["scores"]["overall"] for r in results) / len(results),
    }
    
    print("\n========== è¯„ä¼°ç»“æœ ==========")
    for metric, score in avg_scores.items():
        print(f"{metric}: {score:.2f}")
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    with open("evaluation_results.json", "w", encoding="utf-8") as f:
        json.dump({
            "summary": avg_scores,
            "details": results
        }, f, ensure_ascii=False, indent=2)
    
    return avg_scores

if __name__ == "__main__":
    # åŠ è½½å¾®è°ƒåçš„æ¨¡å‹
    from unsloth import FastLanguageModel
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name="./outputs/final_model",
        max_seq_length=2048,
    )
    
    run_evaluation("data/splits/test.jsonl", model, tokenizer)
```

### 4. é…ç½®æ–‡ä»¶ (`config.yaml`)

```yaml
# æ¨¡å‹å¾®è°ƒé…ç½®

# åŸºåº§æ¨¡å‹
model_name: "Qwen/Qwen2.5-7B"
max_seq_length: 2048

# LoRAé…ç½®
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05

# è®­ç»ƒé…ç½®
batch_size: 2
gradient_accumulation_steps: 8
learning_rate: 2e-4
num_epochs: 3
warmup_ratio: 0.1

# æ•°æ®é…ç½®
system_prompt: "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é‡‘èåˆ†æåŠ©æ‰‹ï¼Œæä¾›å‡†ç¡®ã€ä¸“ä¸šçš„é‡‘èå’¨è¯¢æœåŠ¡ã€‚"
min_output_length: 20
max_output_length: 1500

# è¯„ä¼°é…ç½®
eval_model: "gpt-4o"
eval_samples: 100
```

---

## ğŸ“¦ ä¾èµ– (`requirements.txt`)

```
torch>=2.1.0
transformers>=4.36.0
datasets>=2.16.0
peft>=0.7.0
trl>=0.7.0
unsloth>=2024.1
accelerate>=0.25.0
bitsandbytes>=0.41.0
openai>=1.12.0
pyyaml>=6.0
tqdm>=4.66.0
```

---

## ğŸš€ ä½¿ç”¨æµç¨‹

```bash
# 1. å‡†å¤‡æ•°æ®
python scripts/prepare_data.py --input data/raw --output data/splits

# 2. å¼€å§‹è®­ç»ƒ
python scripts/train.py

# 3. è¯„ä¼°æ¨¡å‹
python scripts/evaluate.py

# 4. å¯¼å‡ºéƒ¨ç½²
python scripts/export.py --format vllm
```

---

## ğŸ’¡ è¡Œä¸šåº”ç”¨åœºæ™¯

| è¡Œä¸š | åº”ç”¨ | æ•°æ®æ¥æº |
|------|------|---------|
| é‡‘è | æŠ•ç ”æŠ¥å‘Šç”Ÿæˆ | ç ”æŠ¥ã€è´¢æŠ¥ |
| åŒ»ç–— | ç—…å†æ‘˜è¦ | è„±æ•ç—…å† |
| æ³•å¾‹ | åˆåŒå®¡æŸ¥ | æ³•å¾‹æ–‡ä¹¦ |
| å®¢æœ | æ™ºèƒ½å›å¤ | å†å²å¯¹è¯ |

---

## ğŸ“Š å­¦ä¹ æ”¶è·

- [x] å¾®è°ƒæ•°æ®å‡†å¤‡æµç¨‹
- [x] LoRAé«˜æ•ˆå¾®è°ƒæŠ€æœ¯
- [x] æ¨¡å‹è¯„ä¼°æ–¹æ³•
- [x] å¾®è°ƒæ¨¡å‹éƒ¨ç½²
