# ğŸš€ æ¨¡å‹è¯„ä¼°ä¸éƒ¨ç½²

> **å­¦ä¹ ç›®æ ‡**ï¼šæŒæ¡å¾®è°ƒæ¨¡å‹çš„è¯„ä¼°æ–¹æ³•å’Œç”Ÿäº§éƒ¨ç½²æµç¨‹

---

## 1. æ¨¡å‹è¯„ä¼°æ–¹æ³•

### 1.1 LLM-as-Judgeè¯„ä¼°

```python
from openai import OpenAI
import json

def llm_evaluate(question: str, reference: str, candidate: str) -> dict:
    """ä½¿ç”¨LLMè¯„ä¼°å›ç­”è´¨é‡"""
    client = OpenAI()
    
    prompt = f"""è¯·è¯„ä¼°ä»¥ä¸‹AIå›ç­”çš„è´¨é‡ã€‚

é—®é¢˜: {question}
å‚è€ƒç­”æ¡ˆ: {reference}
å¾…è¯„ä¼°å›ç­”: {candidate}

è¯·ä»ä»¥ä¸‹ç»´åº¦è¯„åˆ†ï¼ˆ1-5åˆ†ï¼‰ï¼š
1. å‡†ç¡®æ€§: ä¿¡æ¯æ˜¯å¦æ­£ç¡®
2. ç›¸å…³æ€§: æ˜¯å¦åˆ‡é¢˜
3. æµç•…æ€§: è¯­è¨€æ˜¯å¦é€šé¡º
4. å¸®åŠ©æ€§: æ˜¯å¦çœŸæ­£è§£å†³é—®é¢˜

è¿”å›JSONæ ¼å¼ã€‚"""
    
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        response_format={"type": "json_object"}
    )
    
    return json.loads(response.choices[0].message.content)
```

---

## 2. vLLMé«˜æ€§èƒ½éƒ¨ç½²

### 2.1 å®‰è£…

```bash
pip install vllm
```

### 2.2 å¯åŠ¨æœåŠ¡

```bash
python -m vllm.entrypoints.openai.api_server \
    --model /path/to/your/model \
    --port 8000 \
    --tensor-parallel-size 1
```

### 2.3 Pythonè°ƒç”¨

```python
from openai import OpenAI

# vLLMå…¼å®¹OpenAI API
client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="not-needed"
)

response = client.chat.completions.create(
    model="/path/to/your/model",
    messages=[
        {"role": "user", "content": "ä½ å¥½"}
    ]
)
print(response.choices[0].message.content)
```

---

## 3. Dockeréƒ¨ç½²

### 3.1 Dockerfile

```dockerfile
FROM python:3.11-slim

WORKDIR /app

# å®‰è£…ä¾èµ–
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶æ¨¡å‹å’Œä»£ç 
COPY model/ /app/model/
COPY app.py .

EXPOSE 8000

CMD ["python", "-m", "vllm.entrypoints.openai.api_server", \
     "--model", "/app/model", "--port", "8000"]
```

### 3.2 docker-compose.yml

```yaml
version: '3.8'

services:
  llm-api:
    build: .
    ports:
      - "8000:8000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    volumes:
      - ./model:/app/model:ro
```

---

## 4. FastAPIå°è£…

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from openai import OpenAI

app = FastAPI(title="Custom LLM API")

client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="not-needed"
)

class ChatRequest(BaseModel):
    message: str
    history: list = []

class ChatResponse(BaseModel):
    response: str

@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """å¯¹è¯æ¥å£"""
    messages = [{"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šåŠ©æ‰‹ã€‚"}]
    messages.extend(request.history)
    messages.append({"role": "user", "content": request.message})
    
    try:
        response = client.chat.completions.create(
            model="your-model",
            messages=messages
        )
        return ChatResponse(response=response.choices[0].message.content)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

---

## 5. å­¦ä¹ æ£€æŸ¥æ¸…å•

- [ ] èƒ½å¤Ÿä½¿ç”¨LLMè¯„ä¼°æ¨¡å‹è´¨é‡
- [ ] ä¼šä½¿ç”¨vLLMéƒ¨ç½²æ¨¡å‹
- [ ] èƒ½å¤Ÿç”¨Dockerå®¹å™¨åŒ–éƒ¨ç½²
- [ ] ä¼šå°è£…FastAPIæ¥å£

---

## ç»§ç»­å­¦ä¹ 

ğŸ“Œ **Week 9 å­¦ä¹ é¡ºåº**ï¼š
1. âœ… LoRAå¾®è°ƒæŠ€æœ¯
2. âœ… å¾®è°ƒæ•°æ®é›†å‡†å¤‡
3. âœ… æ¨¡å‹è¯„ä¼°ä¸éƒ¨ç½²ï¼ˆæœ¬æ•™ç¨‹ï¼‰
