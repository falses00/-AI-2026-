# ğŸ“˜ ç¬¬9å‘¨ï¼šæ¨¡å‹å¾®è°ƒä¸ä¼˜åŒ–

> **å­¦ä¹ ç›®æ ‡**ï¼šæŒæ¡å¤§æ¨¡å‹å¾®è°ƒæŠ€æœ¯ï¼Œä¼˜åŒ–æ¨¡å‹ä»¥é€‚åº”ç‰¹å®šä¸šåŠ¡åœºæ™¯

---

## ğŸ¯ æœ¬å‘¨ç›®æ ‡

å®Œæˆæœ¬å‘¨å­¦ä¹ åï¼Œä½ å°†èƒ½å¤Ÿï¼š

- âœ… ç†è§£æ¨¡å‹å¾®è°ƒçš„åŸç†ä¸æ–¹æ³•
- âœ… ä½¿ç”¨LoRAè¿›è¡Œé«˜æ•ˆå¾®è°ƒ
- âœ… å‡†å¤‡é«˜è´¨é‡è®­ç»ƒæ•°æ®
- âœ… è¯„ä¼°å¾®è°ƒæ•ˆæœ
- âœ… éƒ¨ç½²å¾®è°ƒåçš„æ¨¡å‹

---

## ğŸ¤” ä¸ºä»€ä¹ˆéœ€è¦å¾®è°ƒï¼Ÿ

| åœºæ™¯ | ç›´æ¥ä½¿ç”¨API | å¾®è°ƒæ¨¡å‹ |
|------|------------|---------|
| é€šç”¨é—®ç­” | âœ… å¤Ÿç”¨ | âŒ ä¸å¿…è¦ |
| ä¸“ä¸šæœ¯è¯­ | âš ï¸ å¯èƒ½ä¸å‡† | âœ… æ›´å‡†ç¡® |
| ç‰¹å®šæ ¼å¼ | âš ï¸ éœ€åå¤è°ƒè¯• | âœ… ç¨³å®šè¾“å‡º |
| ä¼ä¸šé£æ ¼ | âŒ éš¾ä»¥ç»Ÿä¸€ | âœ… ä¸€è‡´æ€§å¥½ |
| æˆæœ¬æ§åˆ¶ | âŒ æŒ‰tokenæ”¶è´¹ | âœ… ä¸€æ¬¡æŠ•å…¥ |

---

## ğŸ“š å­¦ä¹ è·¯å¾„

### Day 1ï¼šå¾®è°ƒåŸºç¡€æ¦‚å¿µ

#### ğŸ“– æ•™ç¨‹ææ–™
- [å¤§æ¨¡å‹å¾®è°ƒå…¥é—¨](./tutorials/01_finetuning_basics.md) ğŸ”œ

**å­¦ä¹ å†…å®¹**ï¼š
- é¢„è®­ç»ƒ vs å¾®è°ƒ
- å…¨é‡å¾®è°ƒ vs å‚æ•°é«˜æ•ˆå¾®è°ƒ
- LoRA/QLoRAåŸç†
- å¾®è°ƒé€‚ç”¨åœºæ™¯

#### å¾®è°ƒæ–¹æ³•å¯¹æ¯”
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    å¾®è°ƒæ–¹æ³•å¯¹æ¯”                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  å…¨é‡å¾®è°ƒ (Full Fine-tuning)                                    â”‚
â”‚  â”œâ”€â”€ æ›´æ–°æ‰€æœ‰å‚æ•°                                               â”‚
â”‚  â”œâ”€â”€ éœ€è¦å¤§é‡GPUæ˜¾å­˜                                            â”‚
â”‚  â””â”€â”€ æ•ˆæœæœ€å¥½ï¼Œä½†æˆæœ¬æœ€é«˜                                        â”‚
â”‚                                                                 â”‚
â”‚  LoRA (Low-Rank Adaptation)                                     â”‚
â”‚  â”œâ”€â”€ åªè®­ç»ƒä½ç§©çŸ©é˜µ                                             â”‚
â”‚  â”œâ”€â”€ æ˜¾å­˜éœ€æ±‚é™ä½80%+                                           â”‚
â”‚  â””â”€â”€ æ•ˆæœæ¥è¿‘å…¨é‡å¾®è°ƒ                                           â”‚
â”‚                                                                 â”‚
â”‚  QLoRA (Quantized LoRA)                                         â”‚
â”‚  â”œâ”€â”€ åŸºç¡€æ¨¡å‹4bité‡åŒ–                                           â”‚
â”‚  â”œâ”€â”€ æ™®é€šGPUå³å¯è®­ç»ƒ                                            â”‚
â”‚  â””â”€â”€ æˆæœ¬æœ€ä½çš„å¾®è°ƒæ–¹æ¡ˆ                                         â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Day 2ï¼šè®­ç»ƒæ•°æ®å‡†å¤‡

#### ğŸ“– æ•™ç¨‹ææ–™
- [å¾®è°ƒæ•°æ®é›†æ„å»º](./tutorials/02_data_preparation.md) ğŸ”œ

**å­¦ä¹ å†…å®¹**ï¼š
- æ•°æ®æ ¼å¼è¦æ±‚
- æ•°æ®æ¸…æ´—æŠ€å·§
- æ•°æ®å¢å¼ºæ–¹æ³•
- è´¨é‡æ£€éªŒæ ‡å‡†

#### ğŸ’» æ•°æ®æ ¼å¼ç¤ºä¾‹
```python
# å¯¹è¯æ ¼å¼ï¼ˆæ¨èï¼‰
training_data = [
    {
        "conversations": [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ³•å¾‹é¡¾é—®"},
            {"role": "user", "content": "ä»€ä¹ˆæ˜¯åŠ³åŠ¨åˆåŒï¼Ÿ"},
            {"role": "assistant", "content": "åŠ³åŠ¨åˆåŒæ˜¯åŠ³åŠ¨è€…ä¸ç”¨äººå•ä½ä¹‹é—´..."}
        ]
    },
    # æ›´å¤šæ ·æœ¬...
]

# æŒ‡ä»¤æ ¼å¼
training_data = [
    {
        "instruction": "å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘æˆè‹±æ–‡",
        "input": "ä»Šå¤©å¤©æ°”å¾ˆå¥½",
        "output": "The weather is nice today"
    },
    # æ›´å¤šæ ·æœ¬...
]
```

---

### Day 3ï¼šLoRAå¾®è°ƒå®æˆ˜

#### ğŸ“– æ•™ç¨‹ææ–™
- [LoRAå¾®è°ƒå®æˆ˜](./tutorials/03_lora_training.md) ğŸ”œ

**å­¦ä¹ å†…å®¹**ï¼š
- ç¯å¢ƒé…ç½®ï¼ˆTransformersã€PEFTï¼‰
- æ¨¡å‹åŠ è½½ä¸é‡åŒ–
- LoRAé…ç½®å‚æ•°
- è®­ç»ƒè¿‡ç¨‹ç›‘æ§

#### ğŸ’» LoRAè®­ç»ƒç¤ºä¾‹
```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model, TaskType
from trl import SFTTrainer

# åŠ è½½åŸºç¡€æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    "deepseek-ai/deepseek-llm-7b-chat",
    load_in_4bit=True,  # QLoRAé‡åŒ–
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("deepseek-ai/deepseek-llm-7b-chat")

# LoRAé…ç½®
lora_config = LoraConfig(
    r=16,                      # LoRAç§©
    lora_alpha=32,             # ç¼©æ”¾å› å­
    target_modules=["q_proj", "v_proj"],  # ç›®æ ‡å±‚
    lora_dropout=0.05,
    bias="none",
    task_type=TaskType.CAUSAL_LM
)

# åº”ç”¨LoRA
model = get_peft_model(model, lora_config)
print(f"å¯è®­ç»ƒå‚æ•°: {model.print_trainable_parameters()}")

# è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir="./lora_output",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    logging_steps=10,
    save_steps=100,
    fp16=True
)

# å¼€å§‹è®­ç»ƒ
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    tokenizer=tokenizer,
    max_seq_length=2048
)
trainer.train()
```

---

### Day 4ï¼šæ¨¡å‹è¯„ä¼°

#### ğŸ“– æ•™ç¨‹ææ–™
- [å¾®è°ƒæ•ˆæœè¯„ä¼°](./tutorials/04_evaluation.md) ğŸ”œ

**å­¦ä¹ å†…å®¹**ï¼š
- è¯„ä¼°æŒ‡æ ‡é€‰æ‹©
- äººå·¥è¯„ä¼°æ–¹æ³•
- è‡ªåŠ¨è¯„ä¼°å·¥å…·
- A/Bæµ‹è¯•è®¾è®¡

#### è¯„ä¼°ç»´åº¦
| ç»´åº¦ | è¯„ä¼°æ–¹æ³• | å·¥å…· |
|------|---------|------|
| å‡†ç¡®æ€§ | æ­£ç¡®ç‡è®¡ç®— | äººå·¥æ ‡æ³¨ |
| æµç•…æ€§ | å›°æƒ‘åº¦(PPL) | è‡ªåŠ¨è®¡ç®— |
| ç›¸å…³æ€§ | ç›¸ä¼¼åº¦è®¡ç®— | BERT Score |
| å®‰å…¨æ€§ | æ•æ„Ÿè¯æ£€æµ‹ | è§„åˆ™+AI |

---

### Day 5ï¼šæ¨¡å‹éƒ¨ç½²

#### ğŸ“– æ•™ç¨‹ææ–™
- [å¾®è°ƒæ¨¡å‹éƒ¨ç½²](./tutorials/05_deployment.md) ğŸ”œ

**å­¦ä¹ å†…å®¹**ï¼š
- æ¨¡å‹åˆå¹¶ä¸å¯¼å‡º
- vLLMé«˜æ€§èƒ½æ¨ç†
- APIæœåŠ¡å°è£…
- æ¨¡å‹ç‰ˆæœ¬ç®¡ç†

#### ğŸ’» æ¨¡å‹åˆå¹¶ç¤ºä¾‹
```python
from peft import PeftModel
from transformers import AutoModelForCausalLM

# åŠ è½½åŸºç¡€æ¨¡å‹
base_model = AutoModelForCausalLM.from_pretrained("deepseek-ai/deepseek-llm-7b-chat")

# åŠ è½½å¹¶åˆå¹¶LoRAæƒé‡
model = PeftModel.from_pretrained(base_model, "./lora_output")
merged_model = model.merge_and_unload()

# ä¿å­˜åˆå¹¶åçš„æ¨¡å‹
merged_model.save_pretrained("./merged_model")
```

---

### Day 6-7ï¼šå®æˆ˜é¡¹ç›®

#### ğŸš€ é¡¹ç›®ï¼šè¡Œä¸šçŸ¥è¯†é—®ç­”æ¨¡å‹

**é¡¹ç›®ç›®æ ‡**ï¼š
å¾®è°ƒä¸€ä¸ªä¸“ä¸šé¢†åŸŸï¼ˆå¦‚æ³•å¾‹ã€åŒ»ç–—ã€é‡‘èï¼‰çš„é—®ç­”æ¨¡å‹

**é¡¹ç›®æ­¥éª¤**ï¼š
1. æ”¶é›†é¢†åŸŸé—®ç­”æ•°æ®ï¼ˆ500-1000æ¡ï¼‰
2. æ•°æ®æ¸…æ´—ä¸æ ¼å¼è½¬æ¢
3. LoRAå¾®è°ƒï¼ˆçº¦2-4å°æ—¶ï¼‰
4. æ•ˆæœè¯„ä¼°ä¸å¯¹æ¯”
5. éƒ¨ç½²ä¸ºAPIæœåŠ¡

---

## ğŸ“º æ¨èèµ„æº

| ç±»å‹ | èµ„æº | é“¾æ¥ |
|------|------|------|
| è§†é¢‘ | LoRAåŸç†è¯¦è§£ | å¾…è¡¥å…… |
| æ–‡æ¡£ | HuggingFace PEFT | https://github.com/huggingface/peft |
| å·¥å…· | LLaMA Factory | https://github.com/hiyouga/LLaMA-Factory |

---

## ğŸ“Š å­¦ä¹ æ£€æŸ¥æ¸…å•

### å¾®è°ƒåŸºç¡€
- [ ] ç†è§£å¾®è°ƒçš„æ„ä¹‰
- [ ] çŸ¥é“LoRA/QLoRAåŸç†
- [ ] äº†è§£ä½•æ—¶éœ€è¦å¾®è°ƒ

### æ•°æ®å‡†å¤‡
- [ ] ä¼šæ„å»ºè®­ç»ƒæ•°æ®é›†
- [ ] èƒ½å¤Ÿè¿›è¡Œæ•°æ®æ¸…æ´—
- [ ] äº†è§£æ•°æ®è´¨é‡æ ‡å‡†

### è®­ç»ƒå®æˆ˜
- [ ] èƒ½å¤Ÿé…ç½®è®­ç»ƒç¯å¢ƒ
- [ ] ä¼šè®¾ç½®LoRAå‚æ•°
- [ ] èƒ½å¤Ÿç›‘æ§è®­ç»ƒè¿‡ç¨‹

### è¯„ä¼°éƒ¨ç½²
- [ ] ä¼šè¯„ä¼°å¾®è°ƒæ•ˆæœ
- [ ] èƒ½å¤Ÿåˆå¹¶æ¨¡å‹æƒé‡
- [ ] ä¼šéƒ¨ç½²å¾®è°ƒæ¨¡å‹

---

## ğŸ¯ ä¸‹ä¸€æ­¥

å®Œæˆæœ¬å‘¨å­¦ä¹ åï¼Œç»§ç»­å‰å¾€ï¼š

ğŸ‘‰ [Week 10: AIäº§å“è®¾è®¡ä¸ç”¨æˆ·ä½“éªŒ](../week10/README.md)

---

**å¾®è°ƒè®©é€šç”¨æ¨¡å‹æˆä¸ºä½ çš„ä¸“å±AIï¼ğŸ’ª**
