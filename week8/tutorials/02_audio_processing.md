# ğŸ¤ è¯­éŸ³å¤„ç†ä¸Whisper

> **å­¦ä¹ ç›®æ ‡**ï¼šæŒæ¡è¯­éŸ³è¯†åˆ«ä¸è¯­éŸ³åˆæˆï¼Œæ„å»ºè¯­éŸ³äº¤äº’åº”ç”¨

---

## 1. è¯­éŸ³AIæŠ€æœ¯æ ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    è¯­éŸ³AIæŠ€æœ¯æ ˆ                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚   è¯­éŸ³è¯†åˆ«       â”‚                   â”‚   è¯­éŸ³åˆæˆ       â”‚      â”‚
â”‚  â”‚   (ASR/STT)     â”‚                   â”‚   (TTS)         â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚           â”‚                                     â”‚               â”‚
â”‚           â–¼                                     â–¼               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  OpenAI Whisper â”‚         â”‚  è¯­éŸ³åˆæˆé€‰é¡¹                 â”‚   â”‚
â”‚  â”‚  - whisper-1    â”‚         â”‚  - OpenAI TTS                â”‚   â”‚
â”‚  â”‚  - whisper-v3   â”‚         â”‚  - Edge TTS (å…è´¹)           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚  - Azure Speech              â”‚   â”‚
â”‚                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                  â”‚
â”‚  åº”ç”¨åœºæ™¯:                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚ ä¼šè®®è½¬å½• â”‚  â”‚ è¯­éŸ³åŠ©æ‰‹â”‚  â”‚ æœ‰å£°ä¹¦  â”‚  â”‚ å®¢æœæœºå™¨äººâ”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. Whisperè¯­éŸ³è¯†åˆ«

### 2.1 ä½¿ç”¨OpenAI API

```python
from openai import OpenAI
from pathlib import Path

client = OpenAI(
    api_key="your-api-key",
    base_url="https://api.openai.com/v1"  # æˆ–å…¶ä»–å…¼å®¹API
)

def transcribe_audio(audio_path: str, language: str = "zh") -> dict:
    """
    è¯­éŸ³è½¬æ–‡å­—
    
    Args:
        audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒmp3, mp4, wav, m4aç­‰ï¼‰
        language: è¯­è¨€ä»£ç ï¼Œzh=ä¸­æ–‡, en=è‹±æ–‡
    
    Returns:
        åŒ…å«è½¬å½•æ–‡æœ¬å’Œå…ƒæ•°æ®çš„å­—å…¸
    """
    with open(audio_path, "rb") as audio_file:
        response = client.audio.transcriptions.create(
            model="whisper-1",
            file=audio_file,
            language=language,
            response_format="verbose_json",  # è·å–è¯¦ç»†ä¿¡æ¯
            timestamp_granularities=["segment", "word"]  # è·å–æ—¶é—´æˆ³
        )
    
    return {
        "text": response.text,
        "language": response.language,
        "duration": response.duration,
        "segments": [
            {
                "start": seg.start,
                "end": seg.end,
                "text": seg.text
            }
            for seg in response.segments
        ]
    }

# ä½¿ç”¨
result = transcribe_audio("meeting.mp3", language="zh")
print(f"è½¬å½•æ–‡æœ¬: {result['text']}")
print(f"æ—¶é•¿: {result['duration']}ç§’")
```

### 2.2 å¸¦æ—¶é—´æˆ³çš„å­—å¹•ç”Ÿæˆ

```python
def generate_srt(audio_path: str, output_path: str):
    """ç”ŸæˆSRTå­—å¹•æ–‡ä»¶"""
    result = transcribe_audio(audio_path)
    
    srt_content = []
    for i, segment in enumerate(result["segments"], 1):
        start_time = format_timestamp(segment["start"])
        end_time = format_timestamp(segment["end"])
        text = segment["text"].strip()
        
        srt_content.append(f"{i}")
        srt_content.append(f"{start_time} --> {end_time}")
        srt_content.append(text)
        srt_content.append("")
    
    with open(output_path, "w", encoding="utf-8") as f:
        f.write("\n".join(srt_content))

def format_timestamp(seconds: float) -> str:
    """å°†ç§’æ•°è½¬æ¢ä¸ºSRTæ—¶é—´æ ¼å¼"""
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    millis = int((seconds % 1) * 1000)
    return f"{hours:02d}:{minutes:02d}:{secs:02d},{millis:03d}"

# ä½¿ç”¨
generate_srt("video.mp4", "subtitles.srt")
```

---

## 3. æœ¬åœ°Whisperæ¨¡å‹

### 3.1 å®‰è£…å¼€æºWhisper

```bash
pip install openai-whisper
# æˆ–æ›´å¿«çš„å®ç°
pip install faster-whisper
```

### 3.2 æœ¬åœ°è½¬å½•

```python
from faster_whisper import WhisperModel

class LocalWhisperService:
    """æœ¬åœ°WhisperæœåŠ¡"""
    
    def __init__(self, model_size: str = "medium"):
        """
        åˆå§‹åŒ–æ¨¡å‹
        
        Args:
            model_size: tiny, base, small, medium, large-v3
        """
        self.model = WhisperModel(
            model_size,
            device="cuda",  # æˆ– "cpu"
            compute_type="float16"  # GPUåŠ é€Ÿ
        )
    
    def transcribe(self, audio_path: str, language: str = "zh") -> dict:
        """è½¬å½•éŸ³é¢‘"""
        segments, info = self.model.transcribe(
            audio_path,
            language=language,
            beam_size=5,
            word_timestamps=True
        )
        
        result_segments = []
        full_text = []
        
        for segment in segments:
            result_segments.append({
                "start": segment.start,
                "end": segment.end,
                "text": segment.text
            })
            full_text.append(segment.text)
        
        return {
            "text": " ".join(full_text),
            "language": info.language,
            "duration": info.duration,
            "segments": result_segments
        }

# ä½¿ç”¨
whisper = LocalWhisperService(model_size="medium")
result = whisper.transcribe("audio.wav")
print(result["text"])
```

---

## 4. è¯­éŸ³åˆæˆ (TTS)

### 4.1 OpenAI TTS

```python
def text_to_speech(
    text: str,
    output_path: str,
    voice: str = "alloy",
    speed: float = 1.0
):
    """
    æ–‡å­—è½¬è¯­éŸ³
    
    Args:
        text: è¦è½¬æ¢çš„æ–‡æœ¬
        output_path: è¾“å‡ºéŸ³é¢‘è·¯å¾„
        voice: å£°éŸ³é€‰é¡¹ (alloy, echo, fable, onyx, nova, shimmer)
        speed: è¯­é€Ÿ (0.25 - 4.0)
    """
    response = client.audio.speech.create(
        model="tts-1",  # æˆ– tts-1-hd é«˜æ¸…ç‰ˆ
        voice=voice,
        input=text,
        speed=speed
    )
    
    response.stream_to_file(output_path)
    print(f"éŸ³é¢‘å·²ä¿å­˜åˆ°: {output_path}")

# ä½¿ç”¨
text_to_speech(
    "æ¬¢è¿å­¦ä¹ AIå·¥ç¨‹å¸ˆè®­ç»ƒè¥ï¼ä»Šå¤©æˆ‘ä»¬æ¥å­¦ä¹ è¯­éŸ³å¤„ç†æŠ€æœ¯ã€‚",
    "welcome.mp3",
    voice="nova"
)
```

### 4.2 Edge TTSï¼ˆå…è´¹æ–¹æ¡ˆï¼‰

```bash
pip install edge-tts
```

```python
import edge_tts
import asyncio

async def edge_text_to_speech(
    text: str,
    output_path: str,
    voice: str = "zh-CN-XiaoxiaoNeural"
):
    """
    ä½¿ç”¨Edge TTSï¼ˆå…è´¹ï¼‰
    
    å¸¸ç”¨ä¸­æ–‡å£°éŸ³:
    - zh-CN-XiaoxiaoNeural (å¥³å£°)
    - zh-CN-YunxiNeural (ç”·å£°)
    - zh-CN-XiaoyiNeural (å¥³å£°)
    """
    communicate = edge_tts.Communicate(text, voice)
    await communicate.save(output_path)

# ä½¿ç”¨
asyncio.run(edge_text_to_speech(
    "è¿™æ˜¯ä¸€ä¸ªå…è´¹çš„è¯­éŸ³åˆæˆæ–¹æ¡ˆï¼Œæ•ˆæœä¹Ÿå¾ˆä¸é”™ï¼",
    "edge_tts_output.mp3"
))

# è·å–æ‰€æœ‰å¯ç”¨å£°éŸ³
async def list_voices():
    voices = await edge_tts.list_voices()
    zh_voices = [v for v in voices if v["Locale"].startswith("zh")]
    for v in zh_voices:
        print(f"{v['ShortName']}: {v['Gender']}")

asyncio.run(list_voices())
```

---

## 5. å®æˆ˜ï¼šè¯­éŸ³å¯¹è¯åŠ©æ‰‹

```python
from openai import OpenAI
import tempfile
import os

class VoiceAssistant:
    """è¯­éŸ³å¯¹è¯åŠ©æ‰‹"""
    
    def __init__(self):
        self.client = OpenAI()
        self.conversation_history = []
    
    async def process_voice(self, audio_path: str) -> str:
        """å¤„ç†è¯­éŸ³è¾“å…¥ï¼Œè¿”å›è¯­éŸ³å›å¤è·¯å¾„"""
        # 1. è¯­éŸ³è½¬æ–‡å­—
        user_text = self.transcribe(audio_path)
        print(f"ç”¨æˆ·è¯´: {user_text}")
        
        # 2. AIå¯¹è¯
        ai_response = self.chat(user_text)
        print(f"AIå›å¤: {ai_response}")
        
        # 3. æ–‡å­—è½¬è¯­éŸ³
        output_path = tempfile.mktemp(suffix=".mp3")
        self.text_to_speech(ai_response, output_path)
        
        return output_path
    
    def transcribe(self, audio_path: str) -> str:
        """è¯­éŸ³è½¬æ–‡å­—"""
        with open(audio_path, "rb") as f:
            response = self.client.audio.transcriptions.create(
                model="whisper-1",
                file=f,
                language="zh"
            )
        return response.text
    
    def chat(self, user_message: str) -> str:
        """AIå¯¹è¯"""
        self.conversation_history.append({
            "role": "user",
            "content": user_message
        })
        
        response = self.client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„è¯­éŸ³åŠ©æ‰‹ï¼Œå›ç­”è¦ç®€æ´ã€‚"},
                *self.conversation_history
            ]
        )
        
        ai_message = response.choices[0].message.content
        self.conversation_history.append({
            "role": "assistant",
            "content": ai_message
        })
        
        return ai_message
    
    def text_to_speech(self, text: str, output_path: str):
        """æ–‡å­—è½¬è¯­éŸ³"""
        response = self.client.audio.speech.create(
            model="tts-1",
            voice="nova",
            input=text
        )
        response.stream_to_file(output_path)

# ä½¿ç”¨
assistant = VoiceAssistant()
reply_audio = await assistant.process_voice("user_input.mp3")
print(f"å›å¤éŸ³é¢‘: {reply_audio}")
```

---

## 6. FastAPIè¯­éŸ³æ¥å£

```python
from fastapi import FastAPI, UploadFile, File
from fastapi.responses import FileResponse
import tempfile
import os

app = FastAPI()
assistant = VoiceAssistant()

@app.post("/api/voice/chat")
async def voice_chat(audio: UploadFile = File(...)):
    """è¯­éŸ³å¯¹è¯æ¥å£"""
    # ä¿å­˜ä¸Šä¼ çš„éŸ³é¢‘
    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as tmp:
        content = await audio.read()
        tmp.write(content)
        input_path = tmp.name
    
    try:
        # å¤„ç†è¯­éŸ³
        output_path = await assistant.process_voice(input_path)
        
        # è¿”å›è¯­éŸ³å›å¤
        return FileResponse(
            output_path,
            media_type="audio/mpeg",
            filename="response.mp3"
        )
    finally:
        os.unlink(input_path)

@app.post("/api/voice/transcribe")
async def transcribe(audio: UploadFile = File(...)):
    """ä»…è¯­éŸ³è½¬æ–‡å­—"""
    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as tmp:
        content = await audio.read()
        tmp.write(content)
        input_path = tmp.name
    
    try:
        text = assistant.transcribe(input_path)
        return {"text": text}
    finally:
        os.unlink(input_path)
```

---

## 7. å­¦ä¹ æ£€æŸ¥æ¸…å•

- [ ] èƒ½å¤Ÿä½¿ç”¨Whisper APIè¿›è¡Œè¯­éŸ³è½¬æ–‡å­—
- [ ] èƒ½å¤Ÿç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„å­—å¹•æ–‡ä»¶
- [ ] ä¼šä½¿ç”¨OpenAI TTSæˆ–Edge TTSè¿›è¡Œè¯­éŸ³åˆæˆ
- [ ] èƒ½å¤Ÿæ„å»ºè¯­éŸ³å¯¹è¯æ¥å£

---

## ç»§ç»­å­¦ä¹ 

ğŸ“Œ **Week 8 å­¦ä¹ é¡ºåº**ï¼š
1. âœ… Visionæ¨¡å‹ä½¿ç”¨
2. âœ… è¯­éŸ³å¤„ç†ä¸Whisperï¼ˆæœ¬æ•™ç¨‹ï¼‰
3. â¡ï¸ å¤šæ¨¡æ€RAGç³»ç»Ÿ
