# ğŸ–¼ï¸ CLIPå›¾åƒEmbeddingä¸å›¾æ–‡æ£€ç´¢

> **å­¦ä¹ ç›®æ ‡**ï¼šæŒæ¡CLIPæ¨¡å‹åŸç†ï¼Œå®ç°å›¾åƒå‘é‡åŒ–å’Œå›¾æ–‡è·¨æ¨¡æ€æ£€ç´¢

---

## 1. CLIPç®€ä»‹

CLIPï¼ˆContrastive Language-Image Pre-trainingï¼‰æ˜¯OpenAIå‘å¸ƒçš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œèƒ½å¤Ÿå°†**å›¾åƒå’Œæ–‡æœ¬**æ˜ å°„åˆ°**åŒä¸€å‘é‡ç©ºé—´**ã€‚

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       CLIP æ¶æ„                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚   å›¾åƒè¾“å…¥                          æ–‡æœ¬è¾“å…¥                   â”‚
â”‚      â”‚                                â”‚                       â”‚
â”‚      â–¼                                â–¼                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚ Vision  â”‚                    â”‚  Text   â”‚                  â”‚
â”‚  â”‚ Encoder â”‚                    â”‚ Encoder â”‚                  â”‚
â”‚  â”‚ (ViT)   â”‚                    â”‚(Transf) â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                  â”‚
â”‚       â”‚                              â”‚                        â”‚
â”‚       â–¼                              â–¼                        â”‚
â”‚  [å›¾åƒå‘é‡]  â†â”€â”€ å¯¹æ¯”å­¦ä¹  â”€â”€â†’  [æ–‡æœ¬å‘é‡]                     â”‚
â”‚       â”‚                              â”‚                        â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                  â–¼                                            â”‚
â”‚           ç»Ÿä¸€å‘é‡ç©ºé—´                                         â”‚
â”‚    (å›¾ç‰‡å’Œæè¿°é è¿‘ï¼Œä¸ç›¸å…³çš„è¿œç¦»)                              â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### CLIPçš„åº”ç”¨åœºæ™¯

| åœºæ™¯ | æè¿° |
|-----|------|
| å›¾åƒåˆ†ç±» | é›¶æ ·æœ¬åˆ†ç±»ï¼Œæ— éœ€è®­ç»ƒ |
| å›¾åƒæ£€ç´¢ | ç”¨æ–‡æœ¬æœç´¢å›¾ç‰‡ |
| å›¾æ–‡åŒ¹é… | åˆ¤æ–­å›¾ç‰‡å’Œæè¿°æ˜¯å¦åŒ¹é… |
| å¤šæ¨¡æ€RAG | å›¾æ–‡æ··åˆçŸ¥è¯†åº“ |

---

## 2. ç¯å¢ƒé…ç½®

```bash
# å®‰è£…OpenAI CLIP
pip install git+https://github.com/openai/CLIP.git

# æˆ–ä½¿ç”¨å¼€æºæ›¿ä»£å“
pip install open-clip-torch

# å®‰è£…å›¾åƒå¤„ç†åº“
pip install pillow
```

---

## 3. åŸºç¡€ä½¿ç”¨

### 3.1 åŠ è½½æ¨¡å‹

```python
import torch
import clip
from PIL import Image

# æ£€æŸ¥è®¾å¤‡
device = "cuda" if torch.cuda.is_available() else "cpu"

# åŠ è½½CLIPæ¨¡å‹
model, preprocess = clip.load("ViT-B/32", device=device)

print(f"æ¨¡å‹åŠ è½½å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: {device}")
```

### 3.2 å›¾åƒç¼–ç 

```python
def encode_image(image_path: str) -> torch.Tensor:
    """å°†å›¾åƒç¼–ç ä¸ºå‘é‡"""
    image = Image.open(image_path)
    image_input = preprocess(image).unsqueeze(0).to(device)
    
    with torch.no_grad():
        image_features = model.encode_image(image_input)
        # å½’ä¸€åŒ–
        image_features = image_features / image_features.norm(dim=-1, keepdim=True)
    
    return image_features

# ç¤ºä¾‹
embedding = encode_image("example.jpg")
print(f"å‘é‡ç»´åº¦: {embedding.shape}")  # [1, 512]
```

### 3.3 æ–‡æœ¬ç¼–ç 

```python
def encode_text(texts: list[str]) -> torch.Tensor:
    """å°†æ–‡æœ¬ç¼–ç ä¸ºå‘é‡"""
    text_tokens = clip.tokenize(texts).to(device)
    
    with torch.no_grad():
        text_features = model.encode_text(text_tokens)
        text_features = text_features / text_features.norm(dim=-1, keepdim=True)
    
    return text_features

# ç¤ºä¾‹
texts = ["ä¸€åªçŒ«", "ä¸€åªç‹—", "ä¸€è¾†æ±½è½¦"]
text_embeddings = encode_text(texts)
print(f"æ–‡æœ¬å‘é‡ç»´åº¦: {text_embeddings.shape}")  # [3, 512]
```

---

## 4. é›¶æ ·æœ¬å›¾åƒåˆ†ç±»

```python
def zero_shot_classify(image_path: str, labels: list[str]) -> dict:
    """é›¶æ ·æœ¬åˆ†ç±»"""
    # ç¼–ç å›¾åƒ
    image_features = encode_image(image_path)
    
    # ç¼–ç æ ‡ç­¾
    text_features = encode_text(labels)
    
    # è®¡ç®—ç›¸ä¼¼åº¦
    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)
    
    # è¿”å›ç»“æœ
    results = {}
    for i, label in enumerate(labels):
        results[label] = float(similarity[0][i])
    
    return dict(sorted(results.items(), key=lambda x: x[1], reverse=True))

# ä½¿ç”¨ç¤ºä¾‹
labels = ["çŒ«", "ç‹—", "é¸Ÿ", "æ±½è½¦", "é£æœº"]
results = zero_shot_classify("pet.jpg", labels)

for label, score in results.items():
    print(f"{label}: {score:.2%}")
```

---

## 5. å›¾åƒæ£€ç´¢

### 5.1 æ„å»ºå›¾åƒç´¢å¼•

```python
import os
from typing import List, Tuple
import numpy as np

class ImageSearchEngine:
    """å›¾åƒæ£€ç´¢å¼•æ“"""
    
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model, self.preprocess = clip.load("ViT-B/32", device=self.device)
        self.image_embeddings: List[np.ndarray] = []
        self.image_paths: List[str] = []
    
    def index_images(self, image_folder: str):
        """ç´¢å¼•æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰å›¾ç‰‡"""
        for filename in os.listdir(image_folder):
            if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.webp')):
                path = os.path.join(image_folder, filename)
                try:
                    embedding = self._encode_image(path)
                    self.image_embeddings.append(embedding)
                    self.image_paths.append(path)
                    print(f"å·²ç´¢å¼•: {filename}")
                except Exception as e:
                    print(f"å¤„ç†å¤±è´¥: {filename} - {e}")
        
        print(f"ç´¢å¼•å®Œæˆï¼Œå…± {len(self.image_paths)} å¼ å›¾ç‰‡")
    
    def _encode_image(self, image_path: str) -> np.ndarray:
        """ç¼–ç å•å¼ å›¾ç‰‡"""
        image = Image.open(image_path).convert("RGB")
        image_input = self.preprocess(image).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            features = self.model.encode_image(image_input)
            features = features / features.norm(dim=-1, keepdim=True)
        
        return features.cpu().numpy()[0]
    
    def search_by_text(self, query: str, top_k: int = 5) -> List[Tuple[str, float]]:
        """ç”¨æ–‡æœ¬æœç´¢å›¾ç‰‡"""
        # ç¼–ç æŸ¥è¯¢æ–‡æœ¬
        text_tokens = clip.tokenize([query]).to(self.device)
        with torch.no_grad():
            text_features = self.model.encode_text(text_tokens)
            text_features = text_features / text_features.norm(dim=-1, keepdim=True)
        
        query_embedding = text_features.cpu().numpy()[0]
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        embeddings_matrix = np.array(self.image_embeddings)
        similarities = np.dot(embeddings_matrix, query_embedding)
        
        # æ’åºè¿”å›Top-K
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        results = []
        for idx in top_indices:
            results.append((self.image_paths[idx], float(similarities[idx])))
        
        return results
    
    def search_by_image(self, query_image_path: str, top_k: int = 5) -> List[Tuple[str, float]]:
        """ç”¨å›¾ç‰‡æœç´¢ç›¸ä¼¼å›¾ç‰‡"""
        query_embedding = self._encode_image(query_image_path)
        
        embeddings_matrix = np.array(self.image_embeddings)
        similarities = np.dot(embeddings_matrix, query_embedding)
        
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        results = []
        for idx in top_indices:
            results.append((self.image_paths[idx], float(similarities[idx])))
        
        return results
```

### 5.2 ä½¿ç”¨ç¤ºä¾‹

```python
# åˆ›å»ºæ£€ç´¢å¼•æ“
engine = ImageSearchEngine()

# ç´¢å¼•å›¾ç‰‡
engine.index_images("./images")

# æ–‡æœ¬æœç´¢
results = engine.search_by_text("æ—¥è½æµ·è¾¹", top_k=5)
for path, score in results:
    print(f"{score:.4f}: {path}")

# ä»¥å›¾æœå›¾
results = engine.search_by_image("query.jpg", top_k=5)
for path, score in results:
    print(f"{score:.4f}: {path}")
```

---

## 6. ä¸ChromaDBé›†æˆ

```python
import chromadb
from chromadb.utils.embedding_functions import OpenCLIPEmbeddingFunction

# ä½¿ç”¨ChromaDBçš„CLIPåµŒå…¥å‡½æ•°
embedding_function = OpenCLIPEmbeddingFunction()

# åˆ›å»ºChromaDBå®¢æˆ·ç«¯
client = chromadb.Client()

# åˆ›å»ºæ”¯æŒå›¾åƒçš„é›†åˆ
collection = client.create_collection(
    name="image_collection",
    embedding_function=embedding_function,
    data_loader=ImageLoader()  # è‡ªå®šä¹‰å›¾åƒåŠ è½½å™¨
)

# æ·»åŠ å›¾åƒ
collection.add(
    ids=["img1", "img2"],
    images=["path/to/image1.jpg", "path/to/image2.jpg"],
    metadatas=[{"category": "cat"}, {"category": "dog"}]
)

# ç”¨æ–‡æœ¬æŸ¥è¯¢
results = collection.query(
    query_texts=["ä¸€åªå¯çˆ±çš„å°çŒ«"],
    n_results=5
)
```

---

## 7. å­¦ä¹ æ£€æŸ¥æ¸…å•

- [ ] ç†è§£CLIPçš„å¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ åŸç†
- [ ] èƒ½å¤Ÿä½¿ç”¨CLIPç¼–ç å›¾åƒå’Œæ–‡æœ¬
- [ ] èƒ½å¤Ÿå®ç°é›¶æ ·æœ¬å›¾åƒåˆ†ç±»
- [ ] èƒ½å¤Ÿæ„å»ºå›¾æ–‡æ£€ç´¢ç³»ç»Ÿ
- [ ] èƒ½å¤Ÿå°†CLIPä¸å‘é‡æ•°æ®åº“é›†æˆ

---

## ç»§ç»­å­¦ä¹ 

ğŸ“Œ **Week 8 å­¦ä¹ é¡ºåº**ï¼š
1. âœ… Visionæ¨¡å‹åŸºç¡€
2. âœ… CLIPå›¾åƒEmbeddingï¼ˆæœ¬æ•™ç¨‹ï¼‰
3. â–¡ è¯­éŸ³å¤„ç†å®æˆ˜
4. â–¡ å¤šæ¨¡æ€RAG

---

**CLIPè®©ä½ çš„ç³»ç»Ÿèƒ½å¤Ÿç†è§£å›¾åƒå’Œæ–‡æœ¬çš„è¯­ä¹‰å…³ç³»ï¼ğŸ–¼ï¸**
