# ğŸ–¼ï¸ å¤šæ¨¡æ€RAGç³»ç»Ÿ

> **å­¦ä¹ ç›®æ ‡**ï¼šæ„å»ºæ”¯æŒå›¾æ–‡æ··åˆçš„æ™ºèƒ½é—®ç­”ç³»ç»Ÿ

---

## 1. å¤šæ¨¡æ€RAGæ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    å¤šæ¨¡æ€RAGæ¶æ„                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                â”‚
â”‚  â”‚  ç”¨æˆ·æŸ¥è¯¢   â”‚ "è¿™å¼ å›¾ç‰‡é‡Œçš„äº§å“æ˜¯ä»€ä¹ˆï¼Ÿ"                       â”‚
â”‚  â”‚ (æ–‡æœ¬+å›¾ç‰‡) â”‚                                                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                                â”‚
â”‚         â”‚                                                        â”‚
â”‚         â–¼                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚              å¤šæ¨¡æ€åµŒå…¥å±‚ (Multimodal Embedding)             â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚ â”‚
â”‚  â”‚  â”‚ æ–‡æœ¬Embed  â”‚              â”‚ å›¾åƒEmbed  â”‚                 â”‚ â”‚
â”‚  â”‚  â”‚ (text-3)   â”‚              â”‚ (CLIP)     â”‚                 â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                 â”‚ â”‚
â”‚  â”‚         â”‚                          â”‚                        â”‚ â”‚
â”‚  â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚ â”‚
â”‚  â”‚                      â–¼                                       â”‚ â”‚
â”‚  â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚ â”‚
â”‚  â”‚              â”‚  ç»Ÿä¸€å‘é‡    â”‚                                â”‚ â”‚
â”‚  â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â”‚                                                        â”‚
â”‚         â–¼                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚              å‘é‡æ£€ç´¢å±‚ (Vector Search)                      â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚ â”‚
â”‚  â”‚  â”‚  æ–‡æ¡£å—    â”‚  â”‚  å›¾ç‰‡æè¿°  â”‚  â”‚  è¡¨æ ¼æ•°æ®  â”‚             â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â”‚                                                        â”‚
â”‚         â–¼                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚              å¤šæ¨¡æ€ç”Ÿæˆå±‚ (GPT-4V / Gemini)                  â”‚ â”‚
â”‚  â”‚  ç»“åˆæ£€ç´¢åˆ°çš„æ–‡æœ¬ã€å›¾ç‰‡ã€è¡¨æ ¼ï¼Œç”Ÿæˆç»¼åˆå›ç­”                   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. å›¾åƒåµŒå…¥ä¸æ£€ç´¢

### 2.1 ä½¿ç”¨CLIPæ¨¡å‹

```bash
pip install transformers torch pillow
```

```python
import torch
from transformers import CLIPProcessor, CLIPModel
from PIL import Image
import numpy as np

class ImageEmbedder:
    """å›¾åƒåµŒå…¥å™¨"""
    
    def __init__(self, model_name: str = "openai/clip-vit-base-patch32"):
        self.model = CLIPModel.from_pretrained(model_name)
        self.processor = CLIPProcessor.from_pretrained(model_name)
        self.model.eval()
    
    def embed_image(self, image_path: str) -> np.ndarray:
        """ç”Ÿæˆå›¾åƒåµŒå…¥å‘é‡"""
        image = Image.open(image_path).convert("RGB")
        inputs = self.processor(images=image, return_tensors="pt")
        
        with torch.no_grad():
            features = self.model.get_image_features(**inputs)
        
        # å½’ä¸€åŒ–
        features = features / features.norm(dim=-1, keepdim=True)
        return features.numpy()[0]
    
    def embed_text(self, text: str) -> np.ndarray:
        """ç”Ÿæˆæ–‡æœ¬åµŒå…¥å‘é‡ï¼ˆç”¨äºå›¾æ–‡åŒ¹é…ï¼‰"""
        inputs = self.processor(text=[text], return_tensors="pt", padding=True)
        
        with torch.no_grad():
            features = self.model.get_text_features(**inputs)
        
        features = features / features.norm(dim=-1, keepdim=True)
        return features.numpy()[0]
    
    def similarity(self, image_path: str, text: str) -> float:
        """è®¡ç®—å›¾æ–‡ç›¸ä¼¼åº¦"""
        img_embed = self.embed_image(image_path)
        text_embed = self.embed_text(text)
        return float(np.dot(img_embed, text_embed))

# ä½¿ç”¨
embedder = ImageEmbedder()
similarity = embedder.similarity("product.jpg", "çº¢è‰²è¿åŠ¨é‹")
print(f"ç›¸ä¼¼åº¦: {similarity:.4f}")
```

---

## 3. å¤šæ¨¡æ€æ–‡æ¡£ç´¢å¼•

### 3.1 æ–‡æ¡£ç»“æ„

```python
from dataclasses import dataclass
from typing import Optional
from enum import Enum

class ContentType(Enum):
    TEXT = "text"
    IMAGE = "image"
    TABLE = "table"

@dataclass
class MultimodalChunk:
    """å¤šæ¨¡æ€å†…å®¹å—"""
    id: str
    content_type: ContentType
    text_content: Optional[str] = None
    image_path: Optional[str] = None
    image_description: Optional[str] = None
    table_data: Optional[list] = None
    embedding: Optional[list] = None
    metadata: dict = None
```

### 3.2 å¤šæ¨¡æ€ç´¢å¼•å™¨

```python
from openai import OpenAI
import chromadb
import uuid

class MultimodalIndexer:
    """å¤šæ¨¡æ€ç´¢å¼•å™¨"""
    
    def __init__(self, collection_name: str = "multimodal_docs"):
        self.client = OpenAI()
        self.image_embedder = ImageEmbedder()
        
        # ChromaDB
        self.chroma = chromadb.Client()
        self.collection = self.chroma.get_or_create_collection(
            name=collection_name,
            metadata={"hnsw:space": "cosine"}
        )
    
    def describe_image(self, image_path: str) -> str:
        """ä½¿ç”¨GPT-4Vç”Ÿæˆå›¾åƒæè¿°"""
        import base64
        
        with open(image_path, "rb") as f:
            image_data = base64.b64encode(f.read()).decode()
        
        response = self.client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": "è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ï¼ŒåŒ…æ‹¬æ‰€æœ‰å¯è§çš„æ–‡å­—ã€ç‰©ä½“ã€é¢œè‰²å’Œå¸ƒå±€ã€‚"},
                        {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}}
                    ]
                }
            ],
            max_tokens=500
        )
        
        return response.choices[0].message.content
    
    def index_text(self, text: str, metadata: dict = None) -> str:
        """ç´¢å¼•æ–‡æœ¬"""
        chunk_id = str(uuid.uuid4())
        
        # ç”Ÿæˆæ–‡æœ¬åµŒå…¥
        response = self.client.embeddings.create(
            model="text-embedding-3-small",
            input=text
        )
        embedding = response.data[0].embedding
        
        self.collection.add(
            ids=[chunk_id],
            embeddings=[embedding],
            documents=[text],
            metadatas=[{"type": "text", **(metadata or {})}]
        )
        
        return chunk_id
    
    def index_image(self, image_path: str, metadata: dict = None) -> str:
        """ç´¢å¼•å›¾åƒ"""
        chunk_id = str(uuid.uuid4())
        
        # ç”Ÿæˆå›¾åƒæè¿°
        description = self.describe_image(image_path)
        
        # ä½¿ç”¨æè¿°ç”Ÿæˆæ–‡æœ¬åµŒå…¥
        response = self.client.embeddings.create(
            model="text-embedding-3-small",
            input=description
        )
        embedding = response.data[0].embedding
        
        self.collection.add(
            ids=[chunk_id],
            embeddings=[embedding],
            documents=[description],
            metadatas=[{
                "type": "image",
                "image_path": image_path,
                **(metadata or {})
            }]
        )
        
        return chunk_id
    
    def search(self, query: str, n_results: int = 5) -> list:
        """æœç´¢ç›¸å…³å†…å®¹"""
        # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
        response = self.client.embeddings.create(
            model="text-embedding-3-small",
            input=query
        )
        query_embedding = response.data[0].embedding
        
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=n_results,
            include=["documents", "metadatas", "distances"]
        )
        
        return [
            {
                "id": results["ids"][0][i],
                "document": results["documents"][0][i],
                "metadata": results["metadatas"][0][i],
                "distance": results["distances"][0][i]
            }
            for i in range(len(results["ids"][0]))
        ]

# ä½¿ç”¨
indexer = MultimodalIndexer()

# ç´¢å¼•æ–‡æœ¬
indexer.index_text("iPhone 15 Proé‡‡ç”¨é’›é‡‘å±è¾¹æ¡†ï¼Œæ­è½½A17 ProèŠ¯ç‰‡")

# ç´¢å¼•å›¾åƒ
indexer.index_image("iphone15_pro.jpg", {"product": "iPhone 15 Pro"})

# æœç´¢
results = indexer.search("è‹¹æœæœ€æ–°æ‰‹æœºæ˜¯ä»€ä¹ˆé…ç½®ï¼Ÿ")
```

---

## 4. å¤šæ¨¡æ€é—®ç­”

```python
import base64
from typing import List

class MultimodalQA:
    """å¤šæ¨¡æ€é—®ç­”ç³»ç»Ÿ"""
    
    def __init__(self):
        self.client = OpenAI()
        self.indexer = MultimodalIndexer()
    
    def answer(
        self,
        question: str,
        image_path: str = None,
        n_context: int = 5
    ) -> str:
        """å›ç­”é—®é¢˜"""
        # 1. æ£€ç´¢ç›¸å…³å†…å®¹
        context_results = self.indexer.search(question, n_results=n_context)
        
        # 2. æ„å»ºä¸Šä¸‹æ–‡
        context_parts = []
        image_contents = []
        
        for result in context_results:
            if result["metadata"]["type"] == "text":
                context_parts.append(f"æ–‡æ¡£ç‰‡æ®µ: {result['document']}")
            elif result["metadata"]["type"] == "image":
                context_parts.append(f"å›¾ç‰‡æè¿°: {result['document']}")
                # è·å–å›¾ç‰‡ç”¨äºGPT-4V
                img_path = result["metadata"].get("image_path")
                if img_path:
                    with open(img_path, "rb") as f:
                        img_data = base64.b64encode(f.read()).decode()
                        image_contents.append({
                            "type": "image_url",
                            "image_url": {"url": f"data:image/jpeg;base64,{img_data}"}
                        })
        
        context = "\n\n".join(context_parts)
        
        # 3. æ„å»ºæ¶ˆæ¯
        messages = [
            {"role": "system", "content": f"""ä½ æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€é—®ç­”åŠ©æ‰‹ã€‚

å‚è€ƒä»¥ä¸‹æ£€ç´¢åˆ°çš„å†…å®¹å›ç­”é—®é¢˜ï¼š

{context}

å¦‚æœæä¾›äº†å›¾ç‰‡ï¼Œè¯·ç»“åˆå›¾ç‰‡å†…å®¹å›ç­”ã€‚"""},
        ]
        
        # ç”¨æˆ·æ¶ˆæ¯
        user_content = [{"type": "text", "text": question}]
        
        # æ·»åŠ ç”¨æˆ·æä¾›çš„å›¾ç‰‡
        if image_path:
            with open(image_path, "rb") as f:
                img_data = base64.b64encode(f.read()).decode()
                user_content.append({
                    "type": "image_url",
                    "image_url": {"url": f"data:image/jpeg;base64,{img_data}"}
                })
        
        # æ·»åŠ æ£€ç´¢åˆ°çš„å›¾ç‰‡
        user_content.extend(image_contents[:3])  # æœ€å¤š3å¼ 
        
        messages.append({"role": "user", "content": user_content})
        
        # 4. ç”Ÿæˆå›ç­”
        response = self.client.chat.completions.create(
            model="gpt-4o",
            messages=messages,
            max_tokens=1000
        )
        
        return response.choices[0].message.content

# ä½¿ç”¨
qa = MultimodalQA()
answer = qa.answer(
    "è¿™æ¬¾æ‰‹æœºçš„ä¸»è¦ç‰¹ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ",
    image_path="phone_photo.jpg"
)
print(answer)
```

---

## 5. å­¦ä¹ æ£€æŸ¥æ¸…å•

- [ ] ç†è§£å¤šæ¨¡æ€åµŒå…¥åŸç†
- [ ] èƒ½å¤Ÿä½¿ç”¨CLIPè¿›è¡Œå›¾æ–‡åŒ¹é…
- [ ] ä¼šæ„å»ºå¤šæ¨¡æ€ç´¢å¼•ç³»ç»Ÿ
- [ ] èƒ½å¤Ÿå®ç°å›¾æ–‡æ··åˆé—®ç­”

---

## ç»§ç»­å­¦ä¹ 

ğŸ“Œ **Week 8 å­¦ä¹ é¡ºåº**ï¼š
1. âœ… Visionæ¨¡å‹ä½¿ç”¨
2. âœ… è¯­éŸ³å¤„ç†ä¸Whisper
3. âœ… å¤šæ¨¡æ€RAGç³»ç»Ÿï¼ˆæœ¬æ•™ç¨‹ï¼‰
